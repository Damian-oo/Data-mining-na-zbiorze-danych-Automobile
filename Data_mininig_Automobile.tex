\documentclass[12pt, a4paper]{article}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % dodatkowe pakiety LaTeX'a
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{indentfirst}

\newcommand{\kb}[1]{\textcolor{red}{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ustawienia globalne



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%\SweaveOpts{concordance=TRUE}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % strona tytulowa
\title{Data mining - kompleksowy \\
projekt na zbiorze danych Automobile}
\author{Damian Lewańczyk}
\date{}
\maketitle
\tableofcontents

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
\section{Analiza eksploracyjna}  
\subsection{Opis danych}
%



Zbiór danych \texttt{automobile data set} złożony jest z $205$ obserwacji (samochodów). Zawiera $25$ różnych zmiennych objaśniających opisujących samochody oraz zmienną objaśnianą \texttt{symboling} - zmienną jakościową, która odpowiada poziomowi ryzyka ubezpieczeniowego samochodu, który odpowiada ich cenie. Jeśli samochód jest bardziej niebezpieczny do ubezpieczenia, ten symbol jest korygowany poprzez jego zwiększenie. Wartości są liczbami naturalnym w zakresie od $-3$ do $+3$. Wartość $+3$ wskazuje, że pojazd jest ryzykowny, a $-3$ wskazuje, że jesteśmy najbardziej skłonni go ubezpieczyć. Zanim przejdziemy do analizy danych zapoznamy się ze zmiennymi. Przedstawimy kolejno każdą z nich podając jej nazwę, typ oraz krótki opis:

\begin{itemize}
\item \textbf{symboling} (zmienna jakościowa, przyjmuje 7 różnych wartości) -  określa poziom ryzyka ryzyka ubezpieczeniowego samochodu
\item \textbf{normalized.losses} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od 65 do 256) - względna średnia wypłata odszkodowania za ubezpieczony pojazd . Ta wartość jest znormalizowana dla wszystkich samochodów w ramach określonej klasyfikacji (dwudrzwiowe małe, kombi, sportowe/specjalne itp.) i reprezentuje średnią stratę na samochód rocznie
\item \textbf{make} (zmienna jakościowa, przyjmuje 22 różne wartości) - marka auta
\item \textbf{fuel.type} (zmienna jakościowa, przyjmuje 2 różne wartości) - typ paliwa dostarczanego silnikowi
\item \textbf{aspiration} (zmienna jakościowa, przyjmuje 2 różne wartości) - zmienna określająca czy silnik jest wolnossący czy ma doładowanie turbo
\item \textbf{num.of.doors} (zmienna jakościowa, przyjmuje 2 różne wartości) - określa czy auto jest dwudrzwiowe czy czterodrzwiowe (W Polsce czasem zamiennie odpowiednio 3d i 5D)
\item \textbf{body.style} (zmienna jakościowa, przyjmuje 5 różnych wartości) - zmienna określająca typ nadwozia
\item \textbf{drive.wheels} (zmienna jakościowa, przyjmuje 3 różne wartości) - opisuje rodzaj napędu
\item \textbf{engine.location} (zmienna jakościowa, przyjmuje 2 różne wartości) - zmienna określająca lokalizacje silnika
\item \textbf{wheel.base} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od $86.6$ do $120.9$) - rozstaw osi, czyli odległość między środkami kół poszczególnych osi, mierzona przy symetrycznym ustawieniu kół względem podłużnej osi pojazdu

\item \textbf{length} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od $141.1$ do $208.1$) - długość pojazdu
\item \textbf{width} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od $60.3$ do $72.3$) - szerokość pojazdu
\item \textbf{heigth} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od $47.8$ do $59.8$) - wysokość pojazdu
\item \textbf{curb.weight} (zmienna ilościowa typu ciągłęgo, przyjmuje wartości od $1488$ do $4066$) - masa własna pojazdu czyli bez pasażerów, bagażu itd.
\item \textbf{engine.type} (zmienna jakościowa, przyjmuje 7 różnych wartości) - typ silnika
\item \textbf{num.of.cylinders} (zmienna jakościowa, przyjmuje 7 różnych wartości) - liczba cylindrów w silniku
\item \textbf{engine.size} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $61$ do $326$) - rozmiar silnika.
\item {fuel.system} (zmienna jakościowa, przyjmuje 8 różnych wartości) - rodzaj systemu paliwowego
\item \textbf{bore} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $2.54$ do $3.96$) - średnica cylindra
\item \textbf{stroke} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $2.07$ do $4.17$) - „Długość skoku”, odległość przebyta przez tłok podczas każdego cyklu
\item \textbf{compression.ratio} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $7$ do $23$) - stopień kompresji, czyli stosunek objętości cylindra do objętości komory spalania w~silniku spalinowym przy ich maksymalnej i minimalnej wartości
\item \textbf{horsepower} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $48$ do $288$) - średnica cylindra
\item \textbf{peak.rpm} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $4150$ do $6600$) - moment obrotowy silnika 
\item \textbf{city.mpg} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $13$ do $49$) - zużycie paliwa podczas jazdy w mieście
\item \textbf{highway.mpg} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $16$ do $54$) - zużycie paliwa podczas jazdy na autostradzie
\item \textbf{price} (zmienna ilościowa typu ciągłego, przyjmuje wartości od $5118$ do $45400$) - wartośc pojazdu



\end{itemize}

\subsection{Przypisanie typów zmiennych}

Przypiszemy teraz manualnie typy zmiennych jakościowych (factor) i ilościowych (numeric) odpowiednim zmiennym, ponieważ domyślnie R myli się przy przypisywaniu. Następnie spojrzymy wstępnie na kilka początkowych wartości każdej zmiennej z naszego zbioru danych za pomocą funkcji \texttt{glimpse} z pakietu \textbf{dplyr}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## Rows: 205
## Columns: 26
## $ symboling         <fct> 3, 3, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0~
## $ normalized.losses <int> NA, NA, NA, 164, 164, NA, 158, NA, 158, NA, 192, 192~
## $ make              <fct> alfa-romero, alfa-romero, alfa-romero, audi, audi, a~
## $ fuel.type         <fct> gas, gas, gas, gas, gas, gas, gas, gas, gas, gas, ga~
## $ aspiration        <fct> std, std, std, std, std, std, std, std, turbo, turbo~
## $ num.of.doors      <fct> two, two, two, four, four, two, four, four, four, tw~
## $ body.style        <fct> convertible, convertible, hatchback, sedan, sedan, s~
## $ drive.wheels      <fct> rwd, rwd, rwd, fwd, 4wd, fwd, fwd, fwd, fwd, 4wd, rw~
## $ engine.location   <fct> front, front, front, front, front, front, front, fro~
## $ wheel.base        <dbl> 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 10~
## $ length            <dbl> 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192~
## $ width             <dbl> 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4~
## $ height            <dbl> 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9~
## $ curb.weight       <int> 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086~
## $ engine.type       <fct> dohc, dohc, ohcv, ohc, ohc, ohc, ohc, ohc, ohc, ohc,~
## $ num.of.cylinders  <fct> four, four, six, four, five, five, five, five, five,~
## $ engine.size       <int> 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 10~
## $ fuel.system       <fct> mpfi, mpfi, mpfi, mpfi, mpfi, mpfi, mpfi, mpfi, mpfi~
## $ bore              <dbl> 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13~
## $ stroke            <dbl> 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40~
## $ compression.ratio <dbl> 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.3~
## $ horsepower        <int> 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 10~
## $ peak.rpm          <int> 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500~
## $ city.mpg          <int> 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, ~
## $ highway.mpg       <int> 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, ~
## $ price             <int> 13495, 16500, 16500, 13950, 17450, 15250, 17710, 189~
\end{verbatim}
\end{kframe}
\end{knitrout}

Następnie dokonamy wstępnego rozeznania w danych za pomocą funkcji \texttt{introduce} i \texttt{plot intro} z pakietu \textbf{DataExplorer}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##                       [,1]
## rows                   205
## columns                 26
## discrete_columns        11
## continuous_columns      15
## all_missing_columns      0
## total_missing_values    59
## complete_rows          159
## total_observations    5330
## memory_usage         41832
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/explor_1-1} 

}


\end{knitrout}

Jak widzimy, odpowiednio przypisaliśmy typy zmiennych w \textbf{R}. Jak możemy zauważyć, w naszych danych są wartości brakujące, którymi zajmiemy się nimi w~następnym podrozdziale.

%---------------------------------------------------------------------------------------------------
\subsection{Szukanie i wypełnianie brakujących wartości}
%---------------------------------------------------------------------------------------------------

Na początek, użyjemy funkcji \texttt{plot missing} z pakietu \textbf{DataExplorer}, aby sprawdzić jakie zmienne oraz jaki ich odsetek ma brakujące wartości.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/explor_NA-1} 

}


\end{knitrout}

Możemy też sprawdzić nie względną, a liczbową wartość wartości brakujących dla każdej kolumny:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{apply}\hlstd{(data,}\hlnum{2}\hlstd{,}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)\{}\hlkwd{length}\hlstd{(}\hlkwd{which}\hlstd{(}\hlkwd{is.na}\hlstd{(x)))\})}
\end{alltt}
\begin{verbatim}
##         symboling normalized.losses              make         fuel.type 
##                 0                41                 0                 0 
##        aspiration      num.of.doors        body.style      drive.wheels 
##                 0                 2                 0                 0 
##   engine.location        wheel.base            length             width 
##                 0                 0                 0                 0 
##            height       curb.weight       engine.type  num.of.cylinders 
##                 0                 0                 0                 0 
##       engine.size       fuel.system              bore            stroke 
##                 0                 0                 4                 4 
## compression.ratio        horsepower          peak.rpm          city.mpg 
##                 0                 2                 2                 0 
##       highway.mpg             price 
##                 0                 4
\end{verbatim}
\end{kframe}
\end{knitrout}
Zamieniamy wartości $NA$ na konkretne liczby tj. na średnie ze zmiennych ilościowych typu ciągłego, a wartości $NA$ zmiennej jakościowej \texttt{num.of.doors} zamienamy na wartości najczęściej występujące, w tym wypadku będzie to wartość \texttt{four}.


\subsection{Wizualizacja danych}

W tym podrozdziale zajmiemy się wizualizacją danych. Na początek zaprezentowane zostaną barploty dla zmiennych jakościowych, które przedstawią częstości występowania poszczególnych wartości dla każdej zmiennej typu factor. Zrobimy to za pomocą funkcji \texttt{plot bar} z pakietu \textbf{DataExplorer}. Widzimy je na rysunkach $1$, $2$ i $3$ poniżej.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_2-1} 

}

\caption[Częstotliwości występowania kategorii dla zmiennych jakościowych]{Częstotliwości występowania kategorii dla zmiennych jakościowych}\label{fig:explor_2-1}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_2-2} 

}

\caption[Częstotliwości występowania kategorii dla zmiennych jakościowych]{Częstotliwości występowania kategorii dla zmiennych jakościowych}\label{fig:explor_2-2}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_2-3} 

}

\caption[Częstotliwości występowania kategorii dla zmiennych jakościowych]{Częstotliwości występowania kategorii dla zmiennych jakościowych}\label{fig:explor_2-3}
\end{figure}

\end{knitrout}

Na podstawie powyższych wykresów, z modelu usuwamy dwie zmienne: \texttt{fuel.type} oraz \texttt{engine.location}, ponieważ występuje w nich za duża dysproporcja między występowaniem klas, przez co nie będą one przekazywać nam praktycznie żadnych informacji. Drugą ważną informacją jest fakt, że nasza jakościowa zmienna objaśniana w ogólnie nie przyjmuje jednej z potencjalnych wartości tj. $-3$, a kolejną wartość tj. $-2$ przyjmuje zaledwie kilka razy. Poza tym, szczególną uwagę trzeba zwrócić na fakt, że jedna ze zmiennych jakościowych - \texttt{make} ma bardzo dużo kategorii (ponad $20$), co może być problemem przy ewentualnych próbach transformacji takiej zmiennej na numeryczną (np. za pomoca One-hot encoding), przez znaczące zwiększenie wymiaru)



Dodatkowo, jest kilka innych zmiennych: \texttt{normalized.losses}, \texttt{engine.type}, \texttt{drive.wheels}, \texttt{fuel.type} i \texttt{aspiration}, które są potencjalnymi kandydatami do usunięcia z modelu. Zmienna  \texttt{normalized.losses}, bo ma duży odsetek wartości brakujących (które na razie zostały zastąpione wartościami średnimi), a reszta zmiennych - która jest typu jakościowego - z powodu dominacji jednej wartości zmiennej. Ostateczną decyzję podejmiemy po dalszej analizie, czy owe zmienne mogą mieć znaczący wpływ na naszą zmienną objaśnianą  \texttt{symboling}. 
\par
Poniżej, na rysunkach $4$, $5$, $6$ i $7$ przedstawione zostały histogramy dla zmiennych ilościowych. Wygenerowane zostały one za pomocą funkcji \texttt{plot histogram} z pakietu \textbf{DataExplorer}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_3-1} 

}

\caption[Histogramy dla zmiennych ilościowych]{Histogramy dla zmiennych ilościowych}\label{fig:explor_3-1}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_3-2} 

}

\caption[Histogramy dla zmiennych ilościowych]{Histogramy dla zmiennych ilościowych}\label{fig:explor_3-2}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_3-3} 

}

\caption[Histogramy dla zmiennych ilościowych]{Histogramy dla zmiennych ilościowych}\label{fig:explor_3-3}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_3-4} 

}

\caption[Histogramy dla zmiennych ilościowych]{Histogramy dla zmiennych ilościowych}\label{fig:explor_3-4}
\end{figure}

\end{knitrout}
Jak możemy zobaczyć, rozkłady zmiennych ilościowych znacząco różnią się kształtem między sobą. I tak, widzimy że np. zmienne \texttt{length}, \texttt{bore} czy \texttt{stroke} mogą pochodzić z rozkładu bardziej przypominającego rozkład normalny, natomiast rozkłady takich zmiennych jak \texttt{horsepower}, \texttt{price} czy \texttt{engine.size} bardziej przypominają np. rozkład wykładniczy. 
\par
Poniżej, na rysunkach $8$, $9$, $10$ i $11$ dodatkowo wygnerowane zostały estymowane gęstości dla zmiennych ilościowych. Stworzone zostały przy pomocy funkcji \texttt{plot density} z pakietu \textbf{DataExplorer}. Wykresy te potwierdzają nasze obserwacje co do różnych charakterystyk zmiennych numerycznych.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_4-1} 

}

\caption[Estymowane gęstości dla zmiennych ilościowych]{Estymowane gęstości dla zmiennych ilościowych}\label{fig:explor_4-1}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_4-2} 

}

\caption[Estymowane gęstości dla zmiennych ilościowych]{Estymowane gęstości dla zmiennych ilościowych}\label{fig:explor_4-2}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_4-3} 

}

\caption[Estymowane gęstości dla zmiennych ilościowych]{Estymowane gęstości dla zmiennych ilościowych}\label{fig:explor_4-3}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_4-4} 

}

\caption[Estymowane gęstości dla zmiennych ilościowych]{Estymowane gęstości dla zmiennych ilościowych}\label{fig:explor_4-4}
\end{figure}

\end{knitrout}




%---------------------------------------------------------------------------------------------------
\subsection{Analiza potencjalnych zależności między zmiennymi}
%---------------------------------------------------------------------------------------------------
\subsubsection {Analiza graficzna zależności między zmienną objaśnianą, a zmiennymi ilościowymi}

Oczywiście najbardziej interesują nas potencjalne zależności między zmienną objaśnianą \texttt{symboling}, a pozostałymi zmiennymi. Dlatego głównie zajmiemy się właśnie nimi, na początek przedstawiając wykresy "pogrupowane" na kategorie wartości zmiennej \texttt{symboling}: wykresy pudełkowe dla zmiennych ilościowych oraz wykresy mozaikowe dla zmiennych jakościowych.

Poniżej, na rysunkach $12$, $13$, $14$ i $15$ przedstawione są wykresy pudełkowe dla zmiennych ilościowych pogrupowane wg. wartości zmiennej \texttt{symboling}. Robimy je przy użyciu funkcji \texttt{plot boxplot} z pakietu \textbf{DataExplorer}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_bp_symbo-1} 

}

\caption[Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling]{Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling}\label{fig:explor_bp_symbo-1}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_bp_symbo-2} 

}

\caption[Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling]{Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling}\label{fig:explor_bp_symbo-2}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_bp_symbo-3} 

}

\caption[Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling]{Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling}\label{fig:explor_bp_symbo-3}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_bp_symbo-4} 

}

\caption[Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling]{Wykresy pudełkowe dla zmiennych ilościowych ze względu na wartości zmiennej symboling}\label{fig:explor_bp_symbo-4}
\end{figure}

\end{knitrout}
Obserwując wykres powyżej, możemy stwierdzić, że praktycznie dla każdej zmiennej numerycznej wykresy pudełkowe dla poszczególnych wartości zmiennej objaśnianej \texttt{symboling} różnią się od siebie. Najbardziej podobne są dla zmiennej \texttt{compression.ratio}, \texttt{peak.rpm} i \texttt{stroke}.  Widzimy też na podstawie tych wykresów, że zależności między zmiennymi objaśniającymi, a zmienną \texttt{symboling} są różne. Jednakże najbliżej zależności, którą nazwalibyśmy liniową (biorąc pod uwagę, że zmienna objaśniana ma kategorie liczbowe i jest porządkowa) są zmienne: \texttt{length}, \texttt{normalized.losses}, \texttt{wheel.base} i \texttt{height}. Zależności między zmienną objaśnianą, a zmiennymi \texttt{length}, \texttt{wheel.base} i \texttt{height} określającymi fizyczne parametry auta są blizko uznania za "malejące", tzn. wraz ze wzrostem któregokolwiek z tych parametrów, zmienna symboling przyjmuje statystycznie mniejsze wartości czyli auto jest mniej ryzykowne. Natomiast odwrotnie jest w przypadku zmiennej \texttt{normalized.losses}, czyli wartości wypłat za pojazd, gdzie im większe jej wartości, tym większa wartość zmiennej \texttt{symboling}, czyli auto jest potencjalnie bardziej ryzykowne.
\par
Na koniec dodajmy, że z racji tego, że zmienna \texttt{normalized.losses} jest tak istotna, oczywiście nie usuwamy jej z modelu, a wartości brakujące zostają z podmienionymi za nie średnimi z reszty wartości.

\subsubsection{Analiza graficzna zależności między zmienną objaśnianą, a zmiennymi jakościowymi}

\par
Poniżej, na rysunkach \ref{fig:explor_mosaic_symbo_1}, \ref{fig:explor_mosaic_symbo_2}, \ref{fig:explor_mosaic_symbo_3} i \ref{fig:explor_mosaic_symbo_5} przedstawione są wykresy mozaikowe dla zmiennych jakościowch pogrupowane wg. wartości  zmiennej \texttt{symboling}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_mosaic_symbo_1-1} 

}

\caption[Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling]{Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling}\label{fig:explor_mosaic_symbo_1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_mosaic_symbo_2-1} 

}

\caption[Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling]{Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling}\label{fig:explor_mosaic_symbo_2}
\end{figure}

\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_mosaic_symbo_3-1} 

}

\caption[Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling]{Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling}\label{fig:explor_mosaic_symbo_3}
\end{figure}

\end{knitrout}

Warto zauważyć, że kolory na legendzie są "do góry nogami" względem tego jak poszczególne kategorie przedstawione są na wykresach. Jest tak na wszystkich wykresach mozaikowych zaprezentowanych tutaj, ale warto na to zwrócić uwagę szczególnie przy następnym, dotyczącym mark aut: 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_mosaic_symbo_5-1} 

}

\caption[Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling]{Częstotliwość występowania poszczególnych kategorii zmiennych jakościowych ze względu na wartości zmiennej symboling}\label{fig:explor_mosaic_symbo_5}
\end{figure}

\end{knitrout}

Na powyższych wykresach jasno widać, że każda zmienna jakościowa ma mniejszy lub większy wpływ na zmienną \texttt{symboling}. Wydaje się bez głębszej analizy, że najmniejszy wpływ ma zmienna \texttt{aspiration}. Zdecydowanie najprostszą do interpretacji jest relacja tej zmiennej ze zmienną \texttt{num.of.doors}. Widzimy, że jeśli auto jest dwudrzwiowe, to wzrasta ryzyko jego ubezpieczenia względem auta czterodrzwiowego. Innym bardzo ciekawym, choć bardzo złożonym wykresem, jest ten ze zmienną \texttt{make}, w przypadku której liczba kategorii znacząco utrudnia opis tego wykresu. Możemy zauważyć, bardzo różne rozkłady kategorii dla poszczególnych wartości zmiennej \texttt{symboling}. Np. wszystkie wartości $-2$ zmiennej \texttt{symboling} są dla marki auta volvo, a reszta volvo ma wartość \texttt{symboling} równą $-1$, natomiast zdecydowana większość aut marki alfa romeo znajduje się w wysokiej grupie ryzyka (\texttt{symboling}=$3$).

\subsubsection{Analiza formalna zależności między zmienną objaśnianą, a zmiennymi ilościowymi}
 
Teraz przejdziemy do bardziej formalnej części analizy zależności. Dla zmiennych ilościowych. tabeli \ref{tab:num_cors} przedstawione zostały wartości odpowiednio $\eta^2$, $\omega^2$ oraz wartości p-value dla testu Kruskala - Walisa. $\eta^2$, $\omega^2$ są typami wielkości efektu \cite{wiki} między zmienną ilościową, a jakościową. Wartości tych współczynników w \textbf{R} zostały obliczone z pomocą funkcji \texttt{anova} oraz \texttt{lm} z~pakietu \textbf{stats} \cite{anovaR}\cite{lmR}. Istnieje wiele różnych interpretacji tych współczynników (od jakiej wartości zmienną możemy uznać za istotną/średnio istotną/ bardzo istotną) - więcej tutaj \cite{eta}, \cite{omega}.  Nam jednakże tak samo bardzo zależy po prostu na uszeregowaniu zmiennych od tych najbardziej istotnych względem zmiennej objaśnianej, do tych najmniej istotnych. Tak samo jeśli chodzi o~wartości p-value dla testu Kruskala-Walisa. Wartości te w \textbf{R} zostały obliczone za pomocą funkcji \texttt{kruskal.test} z pakietu \textbf{stats} \cite{KruskalR}. Test ten nie zakłada normalności rozkładów. Hipotezą zerową jest równość dystrybuant rozkładów w porównywanych populacjach (w naszym wypadku, dla różnych wartości zmiennej \texttt{symboling} tzn. im mniejsza p-value tym teoretycznie większa szansa na to, że dystrybuanty rozkładów są różne dla poszczególnych klas, czyli większa szansa na to, że zmienna ilościowa jest w pewien sposób zależna od jakościowej (i na odwrót). Więcej tutaj \cite{Kruskal}. W tabeli poniżej zmienne zostały uszeregowane

% latex table generated in R 4.3.0 by xtable 1.8-4 package
% Tue Jun 27 00:01:48 2023
\begin{table}[H]
\centering
\begin{tabular}{rlrrr}
  \hline
 & Zmienna ilościowa & \$$\backslash$eta\$ & omega squared & kruskal p value \\ 
  \hline
11 & wheel.base & 0.360 & 0.253 & 3.82E-16 \\ 
  1 & height & 0.350 & 0.247 & 2.29E-13 \\ 
  15 & length & 0.271 & 0.199 & 1.81E-11 \\ 
  2 & normalized.losses & 0.260 & 0.192 & 1.33E-10 \\ 
  8 & curb.weight & 0.249 & 0.184 & 8.80E-12 \\ 
  12 & bore & 0.235 & 0.175 & 3.95E-10 \\ 
  6 & highway.mpg & 0.218 & 0.163 & 5.29E-10 \\ 
  5 & city.mpg & 0.214 & 0.160 & 1.60E-09 \\ 
  3 & horsepower & 0.173 & 0.130 & 3.36E-09 \\ 
  10 & width & 0.158 & 0.118 & 4.71E-09 \\ 
  7 & price & 0.147 & 0.110 & 1.00E-10 \\ 
  4 & peak.rpm & 0.131 & 0.096 & 1.12E-04 \\ 
  9 & engine.size & 0.114 & 0.082 & 1.07E-08 \\ 
  13 & compression.ratio & 0.065 & 0.039 & 1.64E-01 \\ 
  14 & stroke & 0.017 & -0.008 & 2.92E-01 \\ 
   \hline
\end{tabular}
\caption{Porównanie współczynników dla zmiennych ilościowych} 
\label{tab:num_cors}
\end{table}


Jak możemy zauważyc w tabeli powyżej, hierarchia zmiennych jest dokładnie taka sama dla współczynników $\eta^2$, $\omega^2$ oraz bardzo podobna dla wartości p-value (tylko, że tam rosnąco zamiast malejąco). Co więcej możemy stwierdzić, że wyniki w tabeli prowadzą do bardzo podobnych wniosków co analiza graficzna jeśli chodzi o analizę hierarchii zależności między zmiennymi ilościowymi, a zmienną \texttt{symboling} tzn. te same zmienne (\texttt{wheel.base}, \texttt{height}, \texttt{length}, \texttt{normalized.losses}) są w grupie najbardziej istotnych i te same zmienne (\texttt{compression.ratio}, \texttt{stroke}) są w grupie najmniej istotnych. 

\subsubsection{Analiza formalna zależności między zmienną objaśnianą, a zmiennymi jakościowymi}

Natomiast dla porównania miary związku między jakościową zmienną objaśnianą, a innymi zmiennymi jakosciowymi użyliśmy współczynnika V Craméra \cite{cramerV}. W tym celu zastosowaliśmy funkcję \texttt{cramerV} z pakietu \textbf{rcompanion} \cite{cramerVR}. Współczynnik ten przyjmuje wartości od $0$ do $1$, przy czym im większa jego wartość, tym większy związek między poszczególnymi zmiennymi. Współczynniki zostały uszeregowane od największego i przedstawione w tabeli \ref{tab:VCram}.

% latex table generated in R 4.3.0 by xtable 1.8-4 package
% Mon Sep  4 15:35:45 2023
\begin{table}[H]
\centering
\begin{tabular}{rlr}
  \hline
 & Zmienna jakościowa & Współczynnik V Cramera \\ 
  \hline
4 & num.of.doors & 0.696 \\ 
  1 & make & 0.542 \\ 
  2 & body.style & 0.366 \\ 
  8 & fuel.system & 0.321 \\ 
  5 & drive.wheels & 0.307 \\ 
  6 & engine.type & 0.279 \\ 
  3 & aspiration & 0.242 \\ 
  7 & num.of.cylinders & 0.233 \\ 
   \hline
\end{tabular}
\caption{Wartości V Cram} 
\label{tab:VCram}
\end{table}

Jak możemy zauważyć powyżej, również w przypadku zmiennych jakościowych analiza formalna prowadzi do podobnych wniosków co analiza graficzna, mianowicie zmienna \texttt{num.of.doors} faktycznie ma zdecydowanie największą wartość współczynnika, natomiast druga w tej hierarchii zmienna \texttt{make} również ma współczynnik zdecydowanie większy od pozostałych. Dodatkowo możemy zaobserwować, że przypuszczenie o tym, że zmienna \texttt{aspiration} jest mało istotna jest potwierdzone przez jej współczynnik V Cramera, i tego jak blisko najmniejszej wartości się znajduje.

\subsubsection{Zależności między resztą zmiennych}

Poniżej, na rysunku \ref{fig:explor_7}\ zamieszczona zotała tablica korelacji między wszystkimi zmiennymi ilościowymi z mapą ciepła. Stworzona została ona przy pomocy funkcji \texttt{plot corelation} z~pakietu \textbf{DataExplorer}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_7-1} 

}

\caption[Tabela korelacji między zmiennymi ilościowymi]{Tabela korelacji między zmiennymi ilościowymi}\label{fig:explor_7}
\end{figure}

\end{knitrout}

Jak widzimy powyżej, między zmiennymi ilościowymi istnieje bardzo dużo silnych korelacji, najsilniej skorelowane są zmienne dotyczące spalania paliwa w mieście i na autostradzie, dodatkowo widzimy dużo korelacji o wartości bezwzględnej przekraczającej $0.8$, a jeszcze więcej wartości większych od $0.7$. Ponadto możemy stwierdzić, że najmniej skorelowanymi z resztą zmiennych są \texttt{compression.ratio}, \texttt{stroke} i \texttt{normalized.losses} 

Ponadto stworzone zostało kilka scatterplotów między różnymi parami zmiennych ilościowych. Zostały one przedstawione na rysunkach \ref{fig:explor_scatt1}, \ref{fig:explor_scatt2}, \ref{fig:explor_scatt3} i \ref{fig:explor_scatt4}.
%
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_scatt1-1} 

}

\caption[Porównanie zależności między zmiennymi engine.size i price]{Porównanie zależności między zmiennymi engine.size i price}\label{fig:explor_scatt1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_scatt2-1} 

}

\caption[Porównanie zależności między zmiennymi horsepower i price]{Porównanie zależności między zmiennymi horsepower i price}\label{fig:explor_scatt2}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_scatt3-1} 

}

\caption[Porównanie zależności między zmiennymi ehorsepower i highway.mpg]{Porównanie zależności między zmiennymi ehorsepower i highway.mpg}\label{fig:explor_scatt3}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_scatt4-1} 

}

\caption[Porównanie zależności między zmiennymi engine.size i city.mpg]{Porównanie zależności między zmiennymi engine.size i city.mpg}\label{fig:explor_scatt4}
\end{figure}

\end{knitrout}

Na podstawie wykresów powyżej, możemy potwierdzić kilka wartości liczbowych z tablicy korelacji \ref{fig:explor_7}. Dwa pierwsze wykresy, czyli \ref{fig:explor_scatt1}, \ref{fig:explor_scatt2} przedstawiające odpowiednio zależności między ceną, a rozmiarem silnika i ceną, a mocą silnika potwiewrdzają obliczone wartości korelacji - $0.86$ i $0.76$ odpowiednio. Widzimy, że faktycznie wraz ze wzrostem ceny wartości obu tych zmiennych mają tendencję rosnącą. Natomiast jeśli chodzi o wykresy \ref{fig:explor_scatt3} i \ref{fig:explor_scatt4}, przedstawiające zależności między liczbą koni mechanicznych a spalaniem paliwa przez auto na autostradzie oraz w mieście, one również potwierdzają obliczone wartości korelacji, które wynosiły odpowiednio $-0.77$ i $-0.8$. Widzimy, że wraz ze wzrostem liczby koni mechannicznych oba typy spalania mają tendencję spadkową.

Poniżej, na rysunku \ref{fig:explor_dens} przedstawiony został wykres porównujący estymowane gęstości zmiennej ilościowej \texttt{engine.size} ze względu na różne wartości zmiennej jakościowej \texttt{body.style}. Wykres ten pokazuje, że istnieją różnice dla rozkładów rozmiaru silnika w zależności od rodzaju nadwozia.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/explor_dens-1} 

}

\caption[Porównanie gęstości zmiennej engine.size ze względu na różne wartości zmiennej body.style]{Porównanie gęstości zmiennej engine.size ze względu na różne wartości zmiennej body.style}\label{fig:explor_dens}
\end{figure}

\end{knitrout}

\section{Klasyfikacja zmiennej objaśnianej}

Na poczatek, stworzymy podzbiór naszych danych złożony tylko ze zmiennych ilościowych oraz zmiennej objaśnianej \texttt{symboling}. Tworzymy go na potrzeby niektórych modeli, które nie mogą przyjmować zmiennych jakościowych.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data_num} \hlkwb{<-} \hlstd{data[}\hlkwd{c}\hlstd{(}\hlstr{"symboling"}\hlstd{, int_kol, num_kol)]}
\end{alltt}
\end{kframe}
\end{knitrout}


\subsection{KNN}

Na samym starcie zaimplementujemy metodę k-najbliższych sąsiadów dla $k=3$, $k=5$, $k=7$, $k=9$, $k=11$, $k=13$, $k=15$, $k=19$, $k=25$   oraz dla następujących podzbiorów zmiennych:
\begin{itemize}
\item wszystkie zmienne numeryczne
\item $8$ wybranych najbardziej istotnych zmiennych numerycznych: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}, \texttt{curb.weight}, \texttt{bore}, \texttt{city.mpg} i \texttt{highway.mpg}
\item $4$ wybrane najbardziej istotne zmienne numeryczne: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}
\end{itemize}
\par
Do tego celu użyjemy funkcji \texttt{ipredknn} z pakietu \textbf{ipred}. Zbiór treningowy będzie obejmował $0.7$ rozmiaru całej próbki, naszą metryką będzie dokładność, a całą procedurę powtórzymy $100$ razy, żeby uśrednić otrzymane rezultaty, a także sprawdzić rozrzut otrzymanych rezultatów.




Na rysunku \ref{fig:klasyf_KNN_wykres_1} przedstawione zostało porównanie średnich między różnymi wybranymi podzbiorami zmiennych dla każdego z wybranych $k$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_KNN_wykres_1-1} 

}

\caption[Porównanie średnich dokładności dla różnych wyborów cech]{Porównanie średnich dokładności dla różnych wyborów cech}\label{fig:klasyf_KNN_wykres_1}
\end{figure}

\end{knitrout}
Jak możemy zaobserwować powyżej, niezależnie od wybranej wartości $k$, rezultaty zawsze okazywały się jednoznaczne - im więcej zmiennych, tym lepiej dla metody $KNN$ tzn. lepsze mediany dokładności otrzymaliśmy, czyli tak samo jak w przypadku średnich z rysunku \ref{fig:klasyf_KNN_wykres_1}. Ponadto widzimy, że dla każdego z wybranych podzbiorów zmiennych, wartość optymalna $k$~jest inna. Dla wszystkich zmiennych jest to $k=5$, dla podzbioru zawierającego $8$ zmiennych - $k=19$, a dla podzbioru z czterema zmiennymi - $k=13$. Dodatkowo widzimy, że dla dowolnego $k$ dokładność największa jest dla wszystkich zmiennych, a najmniejsza dla podzbioru $4$~zmiennych, zmieniają się tylko różnice między posczególnymi wyborami cech.
\par

Na rysunku \ref{fig:klasyf_KNN_wykres_2} w formie wykresów pudełkowych zostały przedstawione rezultaty naszych symulacji. Czerwone kropki odpowiadają wartościom odpowiednich średnich.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_KNN_wykres_2-1} 

}

\caption[Porównanie dokładności metody KNN dla różnych k i różnych wyborów zmiennych]{Porównanie dokładności metody KNN dla różnych k i różnych wyborów zmiennych}\label{fig:klasyf_KNN_wykres_2}
\end{figure}

\end{knitrout}

Jak możemy zaobserwować na rysunkach powyżej, modele KNN mają bardzo zbliżone rozrzuty niezależnie od wybranego $k$ oraz podzbioru cech. Przeważnie są one największe dla podzbioru $4$ zmiennych, ale nie są to duże różnice. Na wykresach pudełkowych widzimy też bardzo mało wartości odstających, co może świadczyć o stabilności tej metody.


\subsection{Lasy losowe}
Kolejną metodą klasyfikacji użytą przez nas będą lasy losowe. Jest to technika z zakresu uczenia maszynowego oparta na aplikacji metody \texttt{bagging} dla drzew decyzyjnych. Ponadto, przy każdym losowaniu bootstrapowej próby, losowany jest także podzbiór cech, wykorzystywanych przy budowie pojedynczego drzewa. Dla problemów klasyfikacji, przyjęło się, że dobrym wyborem jest losowanie $\sqrt p$, gdzie $p$ oznacza liczbę wszystkich zmiennych w zbiorze danych.  Dużą zaletą tej metody jest fakt, że potrafi ona uzwględniać zarówno zmienne ilościowe, jak i jakościowe - porządkowe oraz nominalne - i to bez potrzeby ich transformacji na ich numeryczne wersje. 

\par Przez następujący błąd:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Error: One or more factor levels in the outcome has no data: '-2' "
\end{verbatim}
\end{kframe}
\end{knitrout}
usuwamy z modelu wiersze z wartością zmiennej \texttt{symboling} równej $-2$ (oraz usuwamy tenże level z tej zmiennej).
\par
Poniżej przedstawiamy wartości częstotliwości występowania odpowiednich kategorii zmiennej \texttt{symboling} po usunięciu z niej wartości $-2$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## 
## -1  0  1  2  3 
## 22 67 54 32 27
\end{verbatim}
\end{kframe}
\end{knitrout}

Na początku stworzymy jeden model z pomocą lasów losowych. Zbiór treniningowy będzie miał wielkość równą $0.7$ rozmiaru naszych danych. Do zbudowania modelu posłużymy się funkcją \texttt{trainControl} z pakietu \textbf{caret} z $5$-krotną walidacją krzyżową oraz funkcją \texttt{randomForest} z pakietu \textbf{randomForest} ze standardową dla tej funkcji liczbą drzew równej $500$.




Na rysunku \ref{fig:klasyf_RF_plot1} zilustrowane zostały błędy w zależności od ilości drzew lasu losowego dla każdej z kategorii zmiennej \texttt{symboling} oraz błąd OOB czyli średni błąd tylko z tych danych, które nie zostały wylosowane do zbioru treningowego. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_RF_plot1-1} 

}

\caption[Błedy dla poszczególnych kategorii w zależności od ilości drzew]{Błedy dla poszczególnych kategorii w zależności od ilości drzew}\label{fig:klasyf_RF_plot1}
\end{figure}

\end{knitrout}

Na rysunku powyżej widzimy, że od pewnego momentu to jest ok. $ntrees=120$ wszystkie błędy, a tym samym wartości dokładności stabilizują się i nie zmieniają się wraz z dodaniem dodatkowych drzewek. Dlatego też będzie to maksymalna liczba drzew dla których będziemy budować i porównywać modele. Jednakże widzimy, że już od dużo mniejszej wartości tego parametru, ok. $20-30$ wartośći błędów, zmieniają się o względnie niewiele dla większości leveli naszej objaśnianej zmiennej jakościowej.

\par
Dodatkowo, model lasu losowego ma tą zaletę, że możemy przy jego pomocy zrobić ranking wszystkich zmiennych wg. ich istotności dla model (warto zwrócić uwagę na fakt, że ranking będzie obejmował na raz zmienne jakościowe i ilościowe). Na rysunku \ref{fig:klasyf_RF_plot2} poniżej przedstawione zostały dwie takie miary. Pierwsza z nich bazuje na sprawdzeniu jak bardzo obniży się dokładność jeśli daną zmienną wykluczymy z modelu, a precyzyjniej mówiąc permutujemy wartości danej zmiennej i sprawdzamy jaki to będzie miało wpływ na skuteczność modelu. Natomiast druga miara okreśła średni spadek czystości przez podziały danej zmiennej. Jeśli zmienna jest użyteczna, ma tendencję do dzielenia mieszanych węzłów z etykietami na węzły czysto pojedynczej kategorii \cite{Gini}, jednakże jest to dla nas mniej ważna miara.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_RF_plot2-1} 

}

\caption[Ranking istoności zmiennych]{Ranking istoności zmiennych}\label{fig:klasyf_RF_plot2}
\end{figure}

\end{knitrout}
Na rysunku powyżej widzimy ranking istotności wszystkich zmiennych, w szczególności zajmiemy się rankingiem po lewej stronie, opisującym potencjalne spadki dokładności przy usuwaniu zmiennych z modelu. Możemy zaobserwować, że najbardziej istotna jest zmienna \texttt{make}, a~najmniej \texttt{aspiration}. Ponadto, widzimy, że ranking ten potwierdza nasze wnioski z rozdziału $1.5$ dotyczące istotności zmienych. Może nie dokładnie odwzorowuje, ale zmienne, które uznaliśmy, że są w grupie najbardziej istotnych, tak samo są i w tej hierarchii. Podobna sytuacja ze zmiennymi najmniej istotnymi.








Poniżej na rysunku \ref{fig:klasyf_RF_plot_ACCURACY} zostały umieszczone wykresy pudełkowe dla dokładności metody lasów losowych pogrupowane ze wzlędu na liczbę użytych drzew w modelu oraz liczbę zmiennych. Różnobarwne kropki na środku każdej z wartości parametru $ntree$ odpowiadają odpowiednim wartościm średnim dla różnych liczb zmiennych. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_RF_plot_ACCURACY-1} 

}

\caption[Porównanie dokładności metody lasów losowych dla różnej liczby drzewek i różnych podzbiorów zmiennych]{Porównanie dokładności metody lasów losowych dla różnej liczby drzewek i różnych podzbiorów zmiennych}\label{fig:klasyf_RF_plot_ACCURACY}
\end{figure}

\end{knitrout}
Jak możemy zaobserwować powyżej, jedynie dla wartości $ntrees=1$, czyli dla pojedynczego drzewa, dokładność jest znacząco gorsza, ale i tu plasuje się na poziomie między $0.6$, a $0.7$ co sprawia, że nawet ta wersja wdrożenia metody daje lepsze rezultaty niż niektóre metody (np. $KNN$ czy $LDA$, którą zaraz omówię). Dla pozostałych wartości $ntree$ wszystkie wartości średnie dokładności oscylują koło $0.8$, na oko w przedziale $(0.77, 0.83)$. Jeśli chodzi o wartości średnie dla różnych podzbiorów zmiennych, to możemy stwierdzić, że dla każdej wartości $ntrees$ najgorzej wypada metoda uwzględniająca wszystkie zmienne. Dla $ntrees=1$ zdecydowanie najlepiej działa metoda uwzględniająca najmniej, czyli $5$ zmiennych, dla $ntrees=20$, $ntrees=60$ i $ntrees=120$ podzbiory biorące pod uwagę $5$, $11$ i $17$ zmiennych zwracają bardzo zbliżone rezultaty. Jeśli chodzi o $IQR$, to możemy stwierdzić, że są one większe dla podzbiorów uwzględniających $17$ i $23$ zmienne, niż dla podzbiorów z $5$ i $11$ zmiennymi, ale różnice nie są szczególnie znaczące.

Poniżej na rysunku \ref{fig:klasyf_RF_plot_TIMES} przedstawione zostały nieuśrednione czasy obliczeniowe dla wszystkich badanych liczb drzewek oraz podzbiorów cech. Pamiętajmy, że w symulacji obliczenia powtarzaliśmy $100$ razy.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_RF_plot_TIMES-1} 

}

\caption[Porównanie czasów obliczeń lasów losowych dla różnej liczby drzewek i różnych podzbiorów zmiennych dla 100 powtórzen w symulacji]{Porównanie czasów obliczeń lasów losowych dla różnej liczby drzewek i różnych podzbiorów zmiennych dla 100 powtórzen w symulacji}\label{fig:klasyf_RF_plot_TIMES}
\end{figure}

\end{knitrout}

Na wykresie powyżej widzimy, że wszystkie czasy obliczeniowe zwiększają się wraz z dodawaniem kolejnych drzewek, co jest jak najbardziej logicznym rezultatem. Dodatkowo - co ciekawe - obserwujemy, że im zmiennych, tym więcej czasu algorytm potrzebował do obliczeń, co na pierwszy rzut oka wydaje się nieoczywiste.




\subsection{LDA}

\par
W tym podrozdziale wykorzystana zostanie liniowa analiza dyskryminacyjna. Z racji charakterystyki tego klasyfikatora, do jego konsturkcji posłużymy się tylko i wyłącznie zmiennymi ilościowymi. Próbowałem przetransformować zmienne jakościowe/ich pewne podzbiory na zmienne numeryczne (one hot encoding), jednak tylko pogarszało to otrzymane rezultaty. Prawdopodobnie przez fakt, zwiększenia $p$ czyli wymiaru danych.

\par
Do tworzenia modeli $LDA$ będziemy posługiwać się funkcją \texttt{lda} z pakietu \textbf{MASS}. Sprawdzimy modele dla:
\begin{itemize}
\item Wszystkich zmiennych numerycznych
\item $8$ zmiennych: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}, \texttt{curb.weight}, \texttt{bore}, \texttt{city.mpg}, \texttt{highway.mpg}
\item $4$ zmiennych: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}
\end{itemize}
Próbki testowe będą standardowo w naszych badaniach miały $0.7$ wielkości zbioru danych. Badaną metryką będzie dokładność, a nasze obliczenia wykonamy $100$-krotnie, a następnie wyniki uśrednimy.



Na rysunku \ref{fig:klasyf_LDA_wykres} przedstawione zostały wykresy pudełkowe dla poszczególnych podzbiorów naszych danych. Czerwone punkty odpowiadają odpowiednim wartościom średnim.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_LDA_wykres-1} 

}

\caption[Porównanie dokładności klasyfikacji LDA]{Porównanie dokładności klasyfikacji LDA}\label{fig:klasyf_LDA_wykres}
\end{figure}

\end{knitrout}

Na podstawie tych wykresów, możemy stwierdzić, że przy ocenie dokładności dla metody $LDA$ najlepiej wypadł najmniejszy podzbiór obejmujący tylko 4 zmienne, a najgorzej największy, obejmujący wszystkie zmienne ilościowe. Oprócz tego, widzimy, że rozrzuty dla poszczególnych podzbiorów są porównywalnej wielkości.

\subsection{QDA}


Jeżeli chodzi o metodę QDA, to po pierwszej nieudanej próbie i błędzie dotyczącym jakiejś za małej grupy, usunąłem z danych te wiersze, w których zmienna \texttt{symboling} przyjmuje wartość $-2$ (były $3$ takie przypadki). Mimo usunięcia tych wierszy oraz wartości $-2$ z poziomów zmiennej jakościowej i tego, że po tej modyfikacji zmienna \texttt{symboling} przyjmowała takie wartości z odpowiednią częstością:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## 
## -1  0  1  2  3 
## 22 67 54 32 27
\end{verbatim}
\end{kframe}
\end{knitrout}
funkcja \texttt{qda} z pakietu \texttt{MASS} dalej zwracała błąd "some group is too small for 'qda' ".


\subsection{Wielomianowa regresja logistyczna}

uwaga: ze względu na wyskakujące ostrzeżenia i wartości odstające, metoda klasyfikacji na bazie wielomianowej regresji logistycznej również wykonana jest na danych z usuniętymi wierszami w których zmienna \texttt{symboling} przymuje wartość $-2$. 
\par Do tworzenia takich modeli użyjemy funkcji \texttt{multinom} z pakietu \textbf{nnet}. Zbudujemy modele dla następujących podbziorów:
\begin{itemize}
\item Wszystkich zmiennych
\item Tylko zmiennych ilościowych
\item $12$ zmiennych: \texttt{make}, \texttt{wheel.base}, \texttt{num.of.doors}, \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{bore}, \texttt{curb.weight}, \texttt{horsepower}, \texttt{city.mpg}, \texttt{highway.mpg}, \texttt{price}
\item $6$ zmiennych: \texttt{make}, \texttt{wheel.base}, \texttt{num.of.doors}, \texttt{normalized.losses}, \texttt{height}, \texttt{length}
\end{itemize}
Zauważmy, że wybrane podzbiory z $12$ i $6$ zmiennymi są mieszanką zmiennych ilościowych i~jakościowych, oczywiście wybraną mniej więcej wg. hierarchii istotnośmi zmiennych.


\par
Poniżej, na rysunk \ref{fig:klasyf_LR_wykres} za pomocą wykresów pudełkowych zostały przedstawione wyniki naszej symulacji z użyciem wielomianowej regresji logistycznej, a odpowiednie czerwone punkty są równe wartościom średnim.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_LR_wykres-1} 

}

\caption[Dokładność klasyfikacji regresji logistycznej]{Dokładność klasyfikacji regresji logistycznej}\label{fig:klasyf_LR_wykres}
\end{figure}

\end{knitrout}
Widzimy na rysunku powyżej, że zdecydowanie najgorzej poradził sobie model w ogólnie nie uwzględniający żadnych zmiennych jakościowych. Możemy przypuszczać, że to przez pominięcie bardzo istotnych zmiennych \texttt{make}, \texttt{num.of.doors}. Poza tym stwierdzamy, że dla "mieszanek" zmiennych jakościowych i ilościowych uwzględnionych w modelu, najlepiej poradził sobie model zawierający najmniej czyli $6$ zmiennych, a najgorzej model uwzględniający wszystkie zmienne. Dodatkowo zauważmy, że dla podzbiorów $6$ i $12$ zmiennych rozrzut jest mniejszy niż w~pozostałych dwóch przypadkach.

\subsection{KDA}

Następną metodą, której użyjemy do klasyfikacji będzie jądrowa analiza dyskryminacyjna czyli $KDA$. Do tworzenia takich modeli użyjemy funkcji \texttt{kda} z pakietu \textbf{ks}. Warto przypomnieć, że metody $KDA$ nie używa się w praktyce dla $p>6$, dlatego my też mocno okroimy nasze dane do $p=3$ oraz że będziemy używać tylko zmiennych ilościowych. Wybraliśmy $3$ następujące podzbiory zmiennych:

\begin{itemize}
\item  \texttt{wheel.base}, \texttt{height}, \texttt{length} ($WHL$)
\item  \texttt{wheel.base}, \texttt{height}, \texttt{normalized.losses} ($WHL$)
\item  \texttt{normalized.losses}, \texttt{curb.weight}, \texttt{bore} ($NCB$)
\end{itemize}

\par
Tak jak wcześniej, będziemy badać dokładność, a losowanie zbioru treningowe oraz obliczenia przeprowadzimy $100$ razy. 
\par Na rysunku \ref{fig:klasyf_KDA_wykres} przedstawione zostały wykresy pudełkowe dla naszych różnych wyborów cech. Czerwone kropki odpowiadają wartościom średnim.



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_KDA_wykres-1} 

}

\caption[Porównanie dokładności klasyfikacji na podstawie KDA]{Porównanie dokładności klasyfikacji na podstawie KDA}\label{fig:klasyf_KDA_wykres}
\end{figure}

\end{knitrout}

Na rysunku powyżej widzimy, że najgorzej poradził sobie podzbiór ($NCB$), i jeśli chodzi o średnią dokładność, i jeśli chodzi o stabilność, co jest dość logiczne, ponieważ wybrane tam zmienne w naszej analizie nie były w czołówce najbardziej istotnych. Dwa pozostałe zbiory poradziły sobie dużo lepiej, w szczególności ($WHL$), który ma najlepszą dokładność oraz najmniejszy rozrzut otrzymanych wyników.


\subsection{Naive Bayes}

Ostatnią metodą zaimplementowaną dla naszych danych będzie naiwny klasyfikator bayesowski, który zakłada, 
że nasze zmienne objaśniające są niezależne. Oczywiście jest to faktycznie bardzo "naiwne" założenie, w rozdziale $1.5.5$ pokazaliśmy, że nasze zmienne są zależne. W tym przypadku również użyjemy danych z pominięciem \texttt{symboling} = $-2$.
Uwzględnimy tylko zmienne numeryczne.

\par

Do tworzenia modeli naiwnego klasyfikatora bayesowskiego będziemy posługiwać się funkcją \texttt{NaiveBayes} z pakietu \textbf{klaR}. Sprawdzimy modele dla:
\begin{itemize}
\item Wszystkich zmiennych numerycznych
\item $8$ zmiennych: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}, \texttt{curb.weight}, \texttt{bore}, \texttt{city.mpg}, \texttt{highway.mpg}
\item $4$ zmiennych: \texttt{normalized.losses}, \texttt{height}, \texttt{length}, \texttt{wheel.base}
\end{itemize}

Tak jak poprzednio, będziemy mierzyć dokładność, a symulacje przeprowadzimy $100$-krotnie dla każdego podzbioru.


\par
Poniżej na rysunku \ref{fig:klasyf_NB_wykres} na wykresach pudełkowych przedstawione zostały wyniki uzyskyskane metodą naiwnego klasyfikatora bayesowskiego, a czerwone punkty sa równe odpowiadającym wartościom średnim.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_NB_wykres-1} 

}

\caption[Porównanie dokładności metodą Naive Bayes]{Porównanie dokładności metodą Naive Bayes}\label{fig:klasyf_NB_wykres}
\end{figure}

\end{knitrout}

Na rysunkach powyżej widzimy, że dokładność naiwnego klasyfikatora bayesowskiego jest tym większa, im mniejsza liczba wybranych cech, a jeśli chodzi o stabilność, to rozrzuty w~poszczególnych grupach są porównywalne.

\section{Podsumowanie I części projektu}
Podsumowując, na samym początku, przy wdrażaniu metod klasyfikacyjnych do konkretnego zagadnienia trzeba pamiętać o ich założeniach, o tym, że niektóre metody takie jak $KNN$ czy $LDA$ obsługują zmienne numeryczne lub o tym że różne metody jak np. $QDA$ czy lasy losowe moga mieć problem jeśli jedna z kategorii naszej zmiennej objaśnianej występuje w niezwykle małej liczbie. Dodatkowo metody mogą mieć inne różne uwarunkowania np. metoda jądrowej analizy dyskryminacyjne w praktyce może być użyta tylko dla małych wymiarów, dla powiedzmy $p<7$.
\par
Po drugie, przy implementacji każdej metody trzeba pamiętać o tym, że dla każdej metody trzeba rozważyć różne parametry (np. $k$ w metodzie k-najbliższych sąsiadów) lub różne podzbiory zmiennych, ponieważ wyniki dla różnych wersji wdrożenia każdej metody mogą być znacząco różne. Dodatkowo, oczywiście należy pamiętać, że dla różnych zbiorów danych o różnej charakterystyce, wyniki tych samych metod moga się różnić.
\par
Po trzecie, trzeba pamiętać o różnorakich miarach oceny naszych algorytmów. W naszej ocenie kierowaliśmy się tylko oceną dokładności, ale w różnych sytuacjach wybraną miarą porównawczą może być np. czułość czy swoistość. Dodatkowo trzeba pamiętać o stabilności otrzymanego rozwiązania - która jest coraz ważniejszym czynnikiem w ocenie metod z zakresu data mining - czyli sprawdzeniu tego jak różne wyniki daje nasza metoda przy niewielkiej zmianie danych.


Na rysunku \ref{fig:klasyf_podsumowanie_wykres} zilustrowane zostało porównanie wykresów pudełkowych dla wszystkich metod. Analizując średnie dokładności oraz stabilności metod, z każdej z nich wybraliśmy jej najlepszą wersję implementacji tj:
\begin{itemize}
\item KNN - Wszystkie zmienne, $k=5$
\item Lasy losowe - $120$ drzewek, $11$ zmiennych
\item LDA - $4$ zmienne
\item LR - $6$ zmiennych
\item KDA - zmienne $WHL$
\item NB $4$ zmienne
\end{itemize}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/klasyf_podsumowanie_wykres-1} 

}

\caption[Porównanie dokładności różnych metod]{Porównanie dokładności różnych metod}\label{fig:klasyf_podsumowanie_wykres}
\end{figure}

\end{knitrout}
Powyżej widzimy, że w naszej analizie porównawczej kilku metod, wyniki jeśli chodzi o~średnie i mediany dokładności są bardzo podobne. Spośród wybranych algorytmów wyraźnie najlepiej poradziły sobie lasy losowe, które jako jedyne uzyskały ponad wartości większe niż $0.8$ jeśli chodzi o średnie i mediany. Kolejnymi najlepszymi metodami były $KDA$ oraz wielomianowa regresja logistyczna, które moga się poszczycić medianami i średnimi większymi od $0.7$, przy czym $KDA$ obie te wartości ma większe. Kolejne metody - $KNN$ oraz naiwny klasyfikator bayesowski mają średnie oraz mediany ponad $0.6$, jednakże naiwny klasyfikator bayesowski obie te metryki miał na lepszym poziomie. Najgorzej w naszym zestawieniu wypadła metoda $LDA$, której nawet najlepsza wersja implementacji nie zdołała uzysać rezultatów na poziomie $0.6$ (dokładniej średnia oraz mediana z zakresu $(0.55, 0.6)$, jednakże trzeba pamiętać, że to i tak dość dobry rezultat przy zagadnieniu klasyfikacji dla $5$ kategorii (w przypadku $LDA$, w przypadku niektórych innych metod - $6$ klas), gdzie dokładność przy losowaniu kategorii zmiennej objaśnianej powinno dawać rezultaty w przybliżeniu $0.16$.




\section{Analiza skupień, klasteryzacja}  

\subsection{Wstęp}
Analiza skupień jest ważnym zagadnieniem w obszarze data mining. Jest narzędziem służącym do eksploaracji danych, jej celem jest ułożenie obiektów w klastry w taki sposób, aby stopień powiązania obiektów z obiektami należącymi do tego samego klastra był jak największy, a z obiektami z innych klastrów jak najmniejszy. Dodatkowo warto wspomnieć, że analiza skupień często jedynie wykrywa struktury w danych, ale nie wyjaśnia dlaczego takie struktury występują.



\par

Jeśli chodzi o nasz zbiór danych, na start usuwamy z niego wiersze, gdzie wartości zmiennej \texttt{symboling} są równe $-2$, robimy tak z powodu zbyt małej liczności takich obserwacji - jest ich zaledwie $3$. (Dodatkowo usuwamy tą wartość z leveli zmiennej typu factor). W analizie skupień nie używa się kategorii zmiennej objaśnianej, dlatego tworzymy dwa podzbiory danych, jeden zawierający wszystkie zmienne poza zmienną \texttt{symboling}, a drugi zawierający wszystkie zmienne ilościowe (tym samym również bez zmiennej objaśnianej \texttt{symboling}). Zbiór danych ze zmiennymi numerycznymi standaryzujemy. Poniżej fragment kodu przedstawiający te transformacje.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data_clust} \hlkwb{<-} \hlstd{data2[}\hlopt{!}\hlstd{(data}\hlopt{$}\hlstd{symboling}\hlopt{==-}\hlnum{2}\hlstd{),]}
\hlstd{data_clust}\hlopt{$}\hlstd{symboling} \hlkwb{<-} \hlkwd{droplevels}\hlstd{(data_clust}\hlopt{$}\hlstd{symboling)}
\hlstd{data_clust_features} \hlkwb{<-} \hlstd{data_clust[,} \hlnum{2}\hlopt{:}\hlnum{26}\hlstd{]}
\hlstd{data_num_clust} \hlkwb{<-} \hlstd{data_clust[}\hlkwd{c}\hlstd{(}\hlstr{"symboling"}\hlstd{, int_kol, num_kol)]}
\hlstd{data_num_clust_features} \hlkwb{<-} \hlstd{data_num_clust[,} \hlnum{2}\hlopt{:}\hlnum{16}\hlstd{]}
\hlstd{data_num_clust_features} \hlkwb{<-} \hlkwd{as.data.frame}\hlstd{(} \hlkwd{scale}\hlstd{(data_num_clust_features) )}
\hlstd{data_clust_symboling_real} \hlkwb{<-} \hlstd{data_clust[,}\hlnum{1}\hlstd{]}
\hlstd{data_num_clust_features_UNSCALED} \hlkwb{<-} \hlstd{data_num_clust[,} \hlnum{2}\hlopt{:}\hlnum{16}\hlstd{]}
\end{alltt}
\end{kframe}
\end{knitrout}

Ponadto, korzystając z faktu, że zmienna \texttt{symboling} jest zmienną porządkową, tworzymy nowy wektor gdzie grupujemy jej wartości: w jednej grupie znajdują się wartości mniejsze, czyli $-1$ i $0$, a w drugiej większe wartości: $1$, $2$ i $3$. Taka konstrukcja przyda nam się w późniejszej części analizy. Poniżej zamieszczony został fragment kodu przedstawiający stworzenie tego wektora.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{symboling_custom}\hlkwb{<-} \hlkwd{as.numeric}\hlstd{(data_clust}\hlopt{$}\hlstd{symboling)}
\hlstd{symboling_custom[}\hlkwd{which}\hlstd{(symboling_custom} \hlopt{%in%} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{))]} \hlkwb{<-} \hlstr{"-1 or 0"}
\hlstd{symboling_custom[}\hlkwd{which}\hlstd{(symboling_custom} \hlopt{%in%} \hlkwd{c}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{4}\hlstd{,}\hlnum{5}\hlstd{))]} \hlkwb{<-} \hlstr{"1 , 2 or 3"}
\end{alltt}
\end{kframe}
\end{knitrout}

Tworzymy jeszcze jedną zmiennej na bazie istniejącej. Mianowicie przeprowadzamy dyskretyzacje zmiennej ciągłej \texttt{price} w następujący sposób: tworzymy $4$ przedziały, które są oddzielane przez liczby $8000$, $12000$ i $18000$. Taka konstrukcja również nam się przyda w późniejszej fazie. Poniżej na rysunku \ref{fig:price_factor} przedstawiony został wykres słupkowy owej zmiennej po takiej transformacji.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/price_factor-1} 

}

\caption[Częstotliwość występowania poszczególnych grup zmiennej \texttt{price}]{Częstotliwość występowania poszczególnych grup zmiennej \texttt{price}}\label{fig:price_factor}
\end{figure}

\end{knitrout}




\subsection{Metody grupujace}

\subsubsection{Metoda k -średnich}
Pierwszą metodą grupującą jakiej użyjemy będzie metoda $k$-średnich ($k-means$). Jest to metoda której na wejściu dostarczamy dane liczbowe (czyli można uwzględnić tylko zmienne numeryczne!), a także $K$ - liczbę klastrów na jaką chcemy podzielić zbiór danych. W inicjalizacji wybieramy $K$ wartości początkowych będących średnimi (centrami skupień). Następnie przyporządkowujemy każdy obiekt do najbliższej mu średniej. Z tak powstałego przyporządkowania wyznaczamy nowe centra skupień (średnie wektorowe). Kroki przyporządkowania i liczenia nowych średnich powtarzamy do spełnienia warunku zbieżności.

\par

Do implementacji metody $k$-means w \textbf{R} posłuży nam funkcja \texttt{kmeans} \cite{kmeans} z pakietu \textbf{stats}. Na rysunku \ref{fig:Plot_kmeans2vars_1} przedstawiona została wizualizacja zastosowania metody $k-means$ na przykładzie zmiennych \texttt{horsepower} i \texttt{highway.mpg} (skrótowo: $HH$) z rozdzieleniem na $K=5$ klastrów z maksymalną liczbą iteracji równą $15$. Liczba klastrów podyktowana jest oczywiście liczbą kategorii objaśnianej zmiennej \texttt{symboling} (po usunięciu z niej kategorii wartości $-2$). Na rysunku zaznaczone sa także większymi, wypełnionymi kwadratami końcowe centra skupień. Ponadto, każda z obserwacji oprócz koloru, ma także przypisany kształt odpowiadający rzeczywistej wartości zmiennej \texttt{symboling}. 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Plot_kmeans2vars_1-1} 

}

\caption[Metoda k-means na dwóch zmiennych]{Metoda k-means na dwóch zmiennych: horsepower i highway.mpg oraz z podziałem na 5 klastrów}\label{fig:Plot_kmeans2vars_1}
\end{figure}

\end{knitrout}

Na rysunku \ref{fig:Plot_kmeans2vars_2} umieszczony został analogiczny wykres, tylko że dla zmiennych \texttt{wheel.base} i \texttt{price} (skrótowo: $WP$).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Plot_kmeans2vars_2-1} 

}

\caption[Metoda k-means na dwóch zmiennych]{Metoda k-means na dwóch zmiennych: wheel.base i price oraz z podziałem na 5 klastrów}\label{fig:Plot_kmeans2vars_2}
\end{figure}

\end{knitrout}

Na obu rysunkach powyżej widzimy jak algorytm $k-means$ stworzył podziały na $5$ grup. Widzimy, że te podziały wyglądaja na sensowne tzn. obszary odpowiadające wszystkim klastrom są rozłączne. W przypadku zmiennych $HH$ klastry są bardziej równoliczne. Co ciekawe, dla tej pary zmiennych widzimy, że obserwacje rozproszone są na wykresie punktowym bardziej równomiernie, niż dla zmiennych $WP$, gdzie punkty są bardziej skoncentrowane w okolicach \texttt{wheel.base}$=-0.3$, \texttt{price}$=-0.7$ niż w obszarach bardziej odległych od tego miejsca. Dlatego też skupienia dla tych zmiennych można ocenić jako mniej zwarte, w przypadku zmiennych $HH$ skupienia są dużo bardziej zwarte dla wszystkich grup, z wyjątkiem $2$ punktów odstających w klastrze o kolorze czerwonym.
\par
Żeby formalniej ocenić lub porównać jakość klasteryzacji, możemy użyć w tym celu różnych wskaźników. Przykładami takich miar są rozrzut wewnątrz skupień (ang. within-cluster scatter) czy rozrzut pomiędzy skupieniami (ang. between-cluster scatter). Tak jak wskazują nazwy, pierwsza z tych miar wskazuje jak duży jest rozrzut między obiektami z tych samych klastrów, a druga ocenia rozrzut między obiektami z różnych klastrów. Poniżej na rysunkach \ref{fig:rozrzut_HH}, \ref{fig:rozrzut_WP} przedstawione zostały porównania obu tych miar oraz totalnego rozrzutu (który jest sumą tych dwóch miar) dla różnych wartości $k$- klastrów w zakresie od $1$ do $10$ dla zmiennych $HH$ oraz $WP$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/rozrzut_HH-1} 

}

\caption[Porównanie różnych rozrzutów dla różnch liczb klastrów]{Porównanie różnych rozrzutów dla różnch liczb klastrów}\label{fig:rozrzut_HH}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/rozrzut_WP-1} 

}

\caption[Porównanie różnych rozrzutów dla różnch liczb klastrów]{Porównanie różnych rozrzutów dla różnch liczb klastrów}\label{fig:rozrzut_WP}
\end{figure}

\end{knitrout}
Możemy zauważyć, że oba wykresy są do siebie bardzo podobne. Co oczywiste wraz ze wzrostem parametru $k$, maleje rozrzut wewnątrz skupień - są bardziej zwarte, a rośnie rzorzut między skupieniami. Widzimy, że dla $k=5$, dla którego zwizualizowane zostały wyżej oba rozrzuty, dla zmiennych $HH$ rozrzut wewnątrz skupień jest równy ok. $50$, a dla zmiennych $WP$ ta wartość jest równa ok. $70$, co potwierdza nasze wnioski z analizy wizualnej, że skupienia w przypadku $HH$ są bardziej zwarte.  

\par
Innym wskaźnikiem służącym do oceny klasteryzacji jest indeks Silhoutte'a \cite{silhoutette}. Wskaźnik ten ocenia każdy obiekt w skali $[-1,1]$, przy czym wartości im bliżej $1$, tym większe prawdopodobieństwo, że obiekt został poprawnie przyporządkowany, a im bliżej $-1$, tym większa szansa, że został on przyporządkowany błędnie. Wartości w okolicach $0$ świadczą o tym, że obiekt lezy pomiędzy klastrami $A$ i $B$. Poniżej na rysunkach \ref{fig:Silhouette_kmeans1} i \ref{fig:Silhouette_kmeans2} widzimy wykresy Silhouette dla metody $k$-means dla odpowiednio zmiennych \texttt{horsepower} i \texttt{highway.mpg} oraz \texttt{wheel.base} i \texttt{price}. Przed każdym wykresem zamieszczony został fragment kodu z wypisanymi indeksami Silhouette'a dla poszczególnych klastrów grupowania.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Silhouette_kmeans1-1} 

}

\caption[Wykres Silhouette zmiennych HH i metody k-means]{Wykres Silhouette zmiennych HH i metody k-means}\label{fig:Silhouette_kmeans1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Średnie wartości indeksów Silhouette dla każdego z klastrów:"
## [1] 0.482 0.506 0.465 0.330 0.502
## [1] "Średnia wartość indeksu Silhouette ogółem: 0.485"
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Silhouette_kmeans2-1} 

}

\caption[Wykres Silhouette dla zmiennych WP i metody k-means]{Wykres Silhouette dla zmiennych WP i metody k-means}\label{fig:Silhouette_kmeans2}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Średnie wartości indeksów Silhouette dla każdego z klastrów:"
## [1] 0.425 0.493 0.268 0.554 0.362
## [1] "Średnia wartość indeksu Silhouette ogółem: 0.448"
\end{verbatim}
\end{kframe}
\end{knitrout}

Z rysunków oraz przedstawionych wartości liczbowych średnich indeksów dla poszczególnych klastrów powyżej, możemy wyciągnąć kilka wniosków. Po pierwsze i najważniejsze, widzimy, że średnia wartość indeksu Silhouette'a dla zmiennych $HH$ jest większa niż dla zmiennych $WP$. Co więcej, widzimy także, że wartości indeksów dla poszczególnych klastrów dla $HH$ mają mniejszy rozrzut np. średnia wartość w tylko jednym klastrze jest mniejsza niż $0.45$. Co więcej, potwierdzają się nasze obserwacje dotyczące tego, że klastry dla przypadku $HH$ są bardziej "równoliczne", mimo jednej mniejszej grupy. Ponadto widzimy, że w przeciwieństwie do zmiennych $WP$, dla $HH$ nie istnieją żadne obiekty, dla których indeks Silhouette'a byłby ujemmy. 

\par
Teraz porównamy średnie indeksy Silhouette'a dla różnych liczb klastrów $k$ oraz dla następujących podzbiorów zmiennych ilościowych:
\begin{itemize}
\item zmienne \texttt{horsepower} i \texttt{highway.mpg} ($HH$)
\item zmienne \texttt{wheel.base} i \texttt{price} ($WP$) 
\item zmienne \texttt{wheel.base}, \texttt{height}, \texttt{length}, \texttt{normalized.losses}, \texttt{curb.weight} ($5$ zmiennych)
\item zmienne \texttt{wheel.base}, \texttt{height}, \texttt{length}, \texttt{normalized.losses}, \texttt{curb.weight}, \texttt{bore}, \texttt{highway.mpg}, \texttt{city.mpg}, \texttt{horsepower} ($9$ zmiennych)
\item wszystkie zmienne numeryczne
\end{itemize}

Wyniki zostały zilustrowane na rysunku \ref{fig:Silhouette_features_kmeans}. Użyliśmy funkcji \texttt{fviz nbclust} \cite{nbclust} z pakietu \textbf{factoextra} z parametrem "FUNcluster = kmeans".
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Silhouette_features_kmeans-1} 

}

\caption[Wykres Silhouette dla różnych podzbiorów zmiennych ilościowych i metody k-means]{Wykres Silhouette dla różnych podzbiorów zmiennych ilościowych i metody k-means}\label{fig:Silhouette_features_kmeans}
\end{figure}

\end{knitrout}

Na podstawie rysunku powyżej możemy wysunąć hipotezę, że dla wszystkich wartości $k$, indeksy Silhouette'a są tym większe, im mniej zmiennych jest branych pod uwagę. Widzimy także, że niezależnie od podzbioru, optymalną liczbą klastrów jest $k=5$. Ponadto, największe wartości indeksu Silhouette'a dla $k=2$ lub $k=3$ osiąga podzbiór zmiennych złożony z \texttt{wheel.base} i \texttt{price}, a dla $k>3$ zmienne \texttt{horsepower}, \texttt{highway.mpg}.

\newpage
 
\subsection{PAM (partition about medoids)}
W tej części porównamy metody $k-means$ i $PAM$, ale najpierw kilka słów wstępu o tym drugim algorytmie. Metoda $PAM$ polega na wybraniu $k$ obiektów będących centrami poszczególnych skupień (tzw. medoidami). $PAM$ tak samo jak metoda $k-means$ potrzebuje mieć z góry podaną liczbę $K$ partycji na ile ma być podzieloby zbiór obiektów, ale w przeciwieństwie do $k-means$, na wejściu dostarczamy nie na dane liczbowe, a dowolną macierz odmienności między obiektami. Co z kolei implikuje, że metody $PAM$ możemy użyć nie tylko dla zmiennych ilościowych, ale także dla zmiennych jakościowych, a także danych mieszanego typu. Wadą może być fakt, że $PAM$ ma większą złożoność obliczeniową - proporcjonalną do $n^2$ ($n$-liczba obiektów) niż $k-means$, gdzie złożonośc jest liniowa. Z tego powodu dla dużych $n$ czas obliczeń algorytmu $PAM$ może być problemem, dlatego dla większych zbiorów danych zaleca się używanie algorytmu $CLARA$ (Clustering LARge Applications). Jednakże w przypadku naszych danych, gdzie $n=205$ (po usunięciu wierszy z \texttt{symboling}$=-2$, $n=202$) będziemy używać zwykłej wersji algorytmu $PAM$.

\par
Jeśli chodzi o implementacje algorytmu $PAM$ w \textbf{R}, zaczniemy od przedstawienia macierzy odmienności dla zmiennych ilościowych, oraz dla wszystkich zmiennych. Możemy ja zauważyć na rysunkach \ref{fig:DM_num} i \ref{fig:DM}. Macierze obliczone są za pomocą funkcji \texttt{daisy} \cite{daisy} z pakietu \textbf{cluster}. Domyślną metryką stosowaną w tej funkcji jest metryka euklidesowa. Jednakże jeśli zostaną wykryte zmienne jakościowe, funkcja \texttt{daisy} automatycznie stosuje miarę odmienności Gowera \cite{Gower}. Wizualizacja macierzy została stworzona z użyciem funkcji \texttt{fviz dist} z pakietu \textbf{factoextra} \cite{factoextra}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/DM_num-1} 

}

\caption[Macierz odmienności dla zmiennych numerycznych]{Macierz odmienności dla zmiennych numerycznych}\label{fig:DM_num}
\end{figure}

\end{knitrout}

Poniżej przedstawiona została wizualizacja macierzy odmienności dla wszystkich zmiennych.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/DM-1} 

}

\caption[Macierz odmienności dla wszystkich zmiennych]{Macierz odmienności dla wszystkich zmiennych}\label{fig:DM}
\end{figure}

\end{knitrout}

\par

Poniżej przedstawiony został wykres dla $PAM$, analogiczny do rysunku \ref{fig:Silhouette_features_kmeans} dotyczącego metody $k-means$. Zobrazowane zostałow porównanie indeksów Silhouette'a dla różnych wartości $k$ z przedziału $[1,10]$ oraz dla różnych podzbiorów zmiennych. 

\begin{itemize}
\item zmienne \texttt{horsepower} i \texttt{highway.mpg} ($HH$)
\item zmienne \texttt{wheel.base} i \texttt{price} ($WP$) 
\item zmienne \texttt{wheel.base}, \texttt{height}, \texttt{length}, \texttt{normalized.losses}, \texttt{curb.weight} ($5$ zmiennych)
\item zmienne \texttt{wheel.base}, \texttt{height}, \texttt{length}, \texttt{normalized.losses}, \texttt{curb.weight}, \texttt{bore}, \texttt{highway.mpg}, \texttt{city.mpg}, \texttt{horsepower} ($9$ zmiennych)
\item wszystkie zmienne numeryczne
\item \texttt{normalized.losses}, \texttt{fuel.system}, \texttt{num.of.doors}, \texttt{length} ($4$ zmienne mix)
\item \texttt{price}, \texttt{city.mpg}, \texttt{curb.weight}, \texttt{body.style}, \texttt{drive.wheels} ($5$ zmiennych mix)
\item \texttt{make}, \texttt{fuel.system}, \texttt{num.of.doors}, \texttt{body.style} ($4$ zmienne factors)
\item \texttt{make}, \texttt{num.of.doors} ($2$ zmienne factors)
\item wszystkie zmienne

\end{itemize}


$5$ podzbiorów, bazujących na zmiennych ilościowych, jest takie samo jak w przypadku metody $k-means$. Kolejne $5$ podzbiorów uwzględnia zmienne jakościowe czy dane mieszanego typu. Ponownie użyliśmy funkcji \texttt{fviz nbclust} \cite{nbclust} z pakietu \textbf{factoextra}, tym razem z parametrem "FUNcluster = cluster::pam", a rezultaty zilustrowane zostały na rysunku \ref{fig:Silhouette_features_PAM}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Silhouette_features_PAM-1} 

}

\caption[Wykres Silhouette dla różnych podzbiorów zmiennych i metody PAM]{Wykres Silhouette dla różnych podzbiorów zmiennych i metody PAM}\label{fig:Silhouette_features_PAM}
\end{figure}

\end{knitrout}

Po pierwsze, na rysunku powyżej widzimy, że $4$ podzbiory zmiennych mają mniejsze indeksy Silhouette'a od pozostałych, są to: "wszystkie zmienne", "zmienne numeryczne", "$5$ zmiennych(num)" i "$9$ zmiennych(num)". Po drugie, w tym przypadku widzimy, że nie zawsze $k=2$ jest optymalną liczbą klastrów. Podzbiory "$4$ zmienne(mix)", "$5$ zmiennych(mix)", "$4$ zmienne(factors)" mają największe wartości indeksów dla odpowiednio $k=6$, $k-7$ i $k=10$. I po trzecie, możemu zauważyć, że największe wartości indeksów dla $k=2$, $k=3$ i $k=10$ są dla podzbioru "$2$ zmienne (factors)".

\par 
Poniżej na rysunku \ref{fig:Silhouette_vs_kmeans_PAM} przedstawione zostało porównanie metod $k-means$ i $PAM$ dla $5$ podzbiorów uwzględniających tylko zmienne ilościowe. Oczywiście odpowiednie dane są te same co na rysunkach \ref{fig:Silhouette_features_kmeans} i \ref{fig:Silhouette_features_PAM}.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Silhouette_vs_kmeans_PAM-1} 

}

\caption[Porównanie indeksów Silhouette'a dla zmiennych WP i metody k-means]{Porównanie indeksów Silhouette'a dla zmiennych WP i metody k-means}\label{fig:Silhouette_vs_kmeans_PAM}
\end{figure}

\end{knitrout}

Na rysunku powyżej, że obie metody zwracają podobne rezultaty jeśli chodzi o indeksy Silhouette'a, niezależnie od wyboru cech czy liczby klastrów. Jednakże widzimy, że dla zdecydowanej większości wypadków lepiej radzi sobie metoda $k-means$ lub obie metody zwracają prawie te same wyniki.

\par
Innymi miarami służącymi do oceny pogrupowania obiektów są wskaźnik Dunn'a \cite{Dunn} i wskaźnik Connectivity. Oba wskaźniki zawsze są nieujemne, różnica polega na tym, że dla wskaźnika Dunn'a im większa wartość, tym lepiej, a dla Connectivity, im mniejsza wartość tym lepiej. W celu obliczeń tych wskaźników posłużyliśmy się funkcjami \texttt{dunn} oraz \texttt{connectivity} z pakietu \textbf{clValid}. Ponadto, w przypadku wskaźnika Connectivity trzeba określić liczbę najbliższych sąsiadów, którą chcemy uwzględnić. W naszym wypadku będzie to $L=10$. Na rysunkach \ref{fig:DI_plot} i \ref{fig:Conn_plot} przedstawione zostały porównania odpowiednio wskaźników Dunn'a oraz Connectivity dla metod $k-means$ i $PAM$ dla podzbiorów zmiennych numerycznych. 



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/DI_plot-1} 

}

\caption[Porównanie wartości indeksu Dunna]{Porównanie wartości indeksu Dunna}\label{fig:DI_plot}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/Conn_plot-1} 

}

\caption[Porównanie wartości wskaźnika connectivity]{Porównanie wartości wskaźnika connectivity}\label{fig:Conn_plot}
\end{figure}

\end{knitrout}

Z rysunków powyżej najważniejszym wnioskiem, niezależnie od interpretacji wartości liczbowych obu wskaźników, które moga się różnić, jest to, że obie metody wypadają kiepsko biorąc pod uwagę te wskaźniki. Widzimy, że dla wskaźnika Dunn'a największą możliwą wartością, którą otrzymaliśmy jest nieco ponad $0.15$, a przypomnijmy, że im większe wartości tym lepiej. Z kolei dla wskaźnika Connectivity otrzymaliśmy duże wartości z minimalną wartością na poziomie ok. $5$, a zdecydowana większośc wartości jest większa od $30$. Ponadto, przy wskaźniku Connectivity możemy jednoznacznie roztrzygnąć, że najlepiej wypadł podzbiór $HH$ czyli zmienne \texttt{horsepower} i \texttt{highway.mpg}.

\subsection{Algorytm DBSCAN}

Kolejną metodą klastrowania, której użyliśmy, jest algorytm $DBSCAN$ (ang. Density-Based Spatial Clustering of Applications with Noise). Jest to jedna z metod grupowania gęstościowego, które nieco różnią się od metod grupujących. Przede wszystkim metody grupowania gęstościowego nie potrzebują mieć z góry nadanej liczby klastrów $k$. Poza tym, są bardziej efektywne w przypadku identyfikacji wartości odstających. Warto dodać, że z racji charakteru algorytmu, oczywiście możemy używać tylko zmiennych ilościowych. Więcej o algorytmie $DBSCAN$ tutaj: \cite{DBSCAN}.
\par
Dla algorytmu $DBSCAN$ musimy ustalić $2$ wejściowe wartości: $\epsilon$ oraz $MinPts$. Należy podkreślić, że jedną z wad metody jest duża wrażliwość na wybór $\epsilon$, szczególnosci w przypadku, gdy poszczególne skupienia maja różną gęstość. Do implementacji algorytmu w \textbf{R} użyliśmy funkcji \texttt{dbscan} z pakietu
\textbf{dbscan}. Poniżej na rysunkach \ref{fig:plot_DBSCAN_CE}, \ref{fig:plot_DBSCAN_WL}, \ref{fig:plot_DBSCAN_CH} i \ref{fig:plot_DBSCAN_CC} przedstawiliśmy wykresy punktowe dla różnych par zmiennych numerycznych z zaimplementowanym algorytmem $DBSCAN$ dla różnych wartości parametrów $\epsilon$ oraz $MinPts$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DBSCAN_CE-1} 

}

\caption[Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych city.mpg i enginze.size]{Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych city.mpg i enginze.size}\label{fig:plot_DBSCAN_CE}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DBSCAN_WL-1} 

}

\caption[Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych wheel.base, length]{Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych wheel.base, length}\label{fig:plot_DBSCAN_WL}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DBSCAN_CH-1} 

}

\caption[Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych compression.ratio, horsepower]{Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych compression.ratio, horsepower}\label{fig:plot_DBSCAN_CH}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DBSCAN_CC-1} 

}

\caption[Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych compression.ratio, curb.weight]{Porównanie działania algorytmu DBSCAN dla różnych parametrów eps oraz minPts dla zmiennych compression.ratio, curb.weight}\label{fig:plot_DBSCAN_CC}
\end{figure}

\end{knitrout}

Jak możemy zaobserwować na rysunkach powyżej, nie otrzymaliśmy satysfakcjonujących rezultatów dla algorytmu $DBSCAN$ dla wybranych par zmiennych, niezależnie od wybranych wartości $\epsilon$ i $minPts$. Prawdopobnie przez bardzo niejednorodną gęstość obserwacji. Oceniając tylko wizualnie, najlepsze wyniki otrzymaliśmy na ostatnim rysunku dla zmiennych \texttt{compression.ratio} i \texttt{curb.weight} dla $\epsilon = 0.5$ i $minPts = 5$.


\subsection{Metody hierarchiczne}

Następnym typem metod których użyliśmy, są metody hierarchiczne. Ich główną ideą jest przedstawienie rezultatów w postaci hierarchii zagnieżdżonych skupień czyli tzw. dendrogramów, podobnie jak w przypadku metod grupowania gęstościowego, tutaj również nie podajemy ustalonej z góry liczb klastrów $k$ na który dzielimy nasz zbiór obiektów, a wyboru dotyczącego tejże liczby dokonujemy po analizie wyników.


\subsubsection{AGNES}
Algorytm $AGNES$ jest przykładem metody aglomeracyjnej w których to na początku każdy obiekt stanowi osobne skupienie, a w następnych krokach najbliższe sobie skupienia są łączone, aż do momentu kiedy uzyskamy jedno skupienie zawierające wszystkie obiekty. Na wejściu algorytm $AGNES$ otrzymuje macierz odmienności, co z kolei sprawia, że możemy wziąć pod uwagę zarówno zmienne ilościowe jak i jakościowe, oraz oczywiście dane mieszanego typu.
\par I właśnie macierzami odmienności zajmiemy się na samym początku, a konkretnie ich wizualizacją. Co prawda, już wcześniej przy metodzie $PAM$ zostały dodane takie rysunki, ale tutaj do naszych heatmap dodamy także dendrogramy oraz faktyczne etykietki zmiennej objaśnianej \texttt{symboling} (warto dodać, że korzystamy w końcu z naszej kosntrukcji zmiennej \texttt{symboling} z $2$ kategoriami). Przedstawione zostały one na rysunkach \ref{fig:plot_Heatmap_1}, \ref{fig:plot_Heatmap_2}, \ref{fig:plot_Heatmap_3} i \ref{fig:plot_Heatmap_4}. Do ich stworzenia użyliśmy funkcji \texttt{heatmap.2} \cite{heatmap2} z pakietu \textbf{gplots}. 








\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_Heatmap_1-1} 

}

\caption[Heatmapa dla macierzy odmienności uwzględniającej zmienne numeryczne]{Heatmapa dla macierzy odmienności uwzględniającej zmienne numeryczne}\label{fig:plot_Heatmap_1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_Heatmap_2-1} 

}

\caption[Heatmapa dla macierzy odmienności uwzględniającej wszystkie zmienne]{Heatmapa dla macierzy odmienności uwzględniającej wszystkie zmienne}\label{fig:plot_Heatmap_2}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_Heatmap_3-1} 

}

\caption[Heatmapa dla macierzy odmienności uwzględniającej zmienne numeryczne (zmienna symboling sprowadzona do 2 kategorii)]{Heatmapa dla macierzy odmienności uwzględniającej zmienne numeryczne (zmienna symboling sprowadzona do 2 kategorii)}\label{fig:plot_Heatmap_3}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_Heatmap_4-1} 

}

\caption[Heatmapa dla macierzy odmienności uwzględniającej wszystkie zmienne (zmienna symboling sprowadzona do 2 kategorii)]{Heatmapa dla macierzy odmienności uwzględniającej wszystkie zmienne (zmienna symboling sprowadzona do 2 kategorii)}\label{fig:plot_Heatmap_4}
\end{figure}

\end{knitrout}

Na podstawie powyższych rysunków, możemy stwierdzić, że otrzymane hierachie zagnieżdżonych skupień kiepsko odwzorowują rzeczywiste kategorie zmiennej \texttt{symboling}.
\par Ważną informacją przy ocenie jakości otrzymanej struktury jest współczynnik aglomeracyjny ($AC$). Im bliżej wartość jest $1$, tym bardziej istotny jest otrzymany podział. Dla każdego obiektu $i$ wyznaczamy $m(i)$, czyli odmienność do pierwszego skupienia, z którym została połączona, a następnie dzielimy ją przed odmienność od skupienia dołączonego w ostatnim kroku algorytmu. Wartość $AC$ to średnia ze wszystkich wartości $1-m(i)$

\par Bardzo ważną sprawą dotyczącą metody $AGNES$ jest wybór metody łączenia klastrów. Istnieją $3$ różne typy takich łączeń:
\begin{itemize}
\item Odległość najbliższego sąsiada (ang. simple linkage)
\item Odległość najdalszego sąsiada (ang. complete linkage)
\item Odległość średnia (ang. average linkage)
\end{itemize}
Ale nie są to jedyne metody. Najpopularniejszą z pozostałych jest prawdopodobnie metoda Warda. Więcej o metodach łączenia klastrów możemy znaleźć tutaj \cite{linkage}.
\par Żeby zwizualizować jak ważny jest wybór metody łączenia klastrów poniżej na rysunkach \ref{fig:plot_AGNES_1.2}, \ref{fig:plot_AGNES_2}, \ref{fig:plot_AGNES_3} przedstawione zostały dentrogramy metody $AGNES$ uzytej na zmiennych ilościowych dla odpowiednio \texttt{average linkage}, \texttt{simple linkage} i \texttt{complete linkage}. Do ich stworzenia użyliśmy funckji \texttt{agnes} \cite{agnes} z pakietu \textbf{cluster}, a następnie funkcji \texttt{fviz dend} \cite{fvizdend} z pakietu \textbf{factoextra} Oprócz tego dla każdego z typów łączeń wyświetlony został także współczynnik $AC$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Współczynnik AC dla metody łączenia average linkage wynosi: 0.87"
\end{verbatim}
\end{kframe}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_1_2-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem average linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem average linkage}\label{fig:plot_AGNES_1.2}
\end{figure}

\end{knitrout}

\newpage

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Współczynnik AC dla metody łączenia single linkage wynosi: 0.81"
\end{verbatim}
\end{kframe}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_2-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem single linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem single linkage}\label{fig:plot_AGNES_2}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Współczynnik AC dla metody łączenia complete linkage wynosi: 0.93"
\end{verbatim}
\end{kframe}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_3-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage}\label{fig:plot_AGNES_3}
\end{figure}

\end{knitrout}

Z $3$ powyższych rysunków, możemy wywnioskować, że najbardziej sensownym podziałem jest ten z pomocą metody łączenie \texttt{complete linkage}. Potwierdza to też najwyższa wartość współczynnika $AC$, właśnie dla tej metody, który jest równy $0.93$. I właśnie dendrogramów przy użyciu tej metody łączenia uzyjemy do porównania ze zmiennymi jakościowymi. Poniżej na rysunkach \ref{fig:plot_AGNES_Comparison_Symboling} i \ref{fig:plot_AGNES_Comparison_Symboling_2} przedstawione są dendrogramy dla metody AGNES ($complete linkage$) na podstawie zmiennych ilościowych z zaznaczonymi etykietkami zmiennej \texttt{symboling}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_Symboling-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej symboling]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej symboling}\label{fig:plot_AGNES_Comparison_Symboling}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_Symboling_2-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej symboling (podział na 2 kategorie)]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej symboling (podział na 2 kategorie)}\label{fig:plot_AGNES_Comparison_Symboling_2}
\end{figure}

\end{knitrout}

Poniżej na rysunkach \ref{fig:plot_AGNES_Comparison_Symboling_all} i \ref{fig:plot_AGNES_Comparison_Symboling_2_all} przedstawione są dendrogramy dla metody AGNES ($complete linkage$), tym razem na podstawie wszystkich zmiennych, także jakościowych (oczywiście oprócz zmiennej objaśnianej \texttt{symboling}) z zaznaczonymi etykietkami zmiennej \texttt{symboling}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_Symboling_all-1} 

}

\caption[dendrogram dla metody AGNES dla wszystkich zmiennych z zaznaczeniem różnych kategorii zmiennej symboling]{dendrogram dla metody AGNES dla wszystkich zmiennych z zaznaczeniem różnych kategorii zmiennej symboling}\label{fig:plot_AGNES_Comparison_Symboling_all}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_Symboling_2_all-1} 

}

\caption[dendrogram dla metody AGNES dla wszystkich zmiennych z zaznaczeniem różnych kategorii zmiennej symboling(podział na 2 kategorie)]{dendrogram dla metody AGNES dla wszystkich zmiennych z zaznaczeniem różnych kategorii zmiennej symboling(podział na 2 kategorie)}\label{fig:plot_AGNES_Comparison_Symboling_2_all}
\end{figure}

\end{knitrout}
Na $4$ powyższych rysunkach możemy zaobserwować, że niezależnie od użycia wszystkich zmiennych czy tylko ilościowych, zagnieżdżenia uzyskiwane metodą $AGNES$ nie odpowiadają wyraźnie jakiemuś podziałowi związanemu ze zmienną \texttt{symboling}


\par
Poniżej na rysunkach \ref{fig:plot_AGNES_Comparison_drive.wheels}, \ref{fig:plot_AGNES_Comparison_num.of.cylinders} i \ref{fig:plot_AGNES_Comparison_fuel.system}  przedstawione zostały dendrogramy uzyskane metodą $AGNES$ dla metody łączenia klastrów $complete linkage$, na podstawie zmiennych ilościowych, z porównaniem z różnymi innymi zmiennymi jakościowymi: \texttt{drive.wheels}, \texttt{num.of.cylinders} i \texttt{fuel.system}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_drive_wheels-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej drive.wheels]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej drive.wheels}\label{fig:plot_AGNES_Comparison_drive.wheels}
\end{figure}

\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_num_of_cylinders-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej num.of.cylinders]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej num.of.cylinders}\label{fig:plot_AGNES_Comparison_num.of.cylinders}
\end{figure}

\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_fuel_system-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej fuel.system]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej fuel.system}\label{fig:plot_AGNES_Comparison_fuel.system}
\end{figure}

\end{knitrout}

Na powyższych rysunkach widzimy, że etykietki każdej ze zmiennych \texttt{drive.wheels}, \texttt{num.of.cylinders} i \texttt{fuel.system} w jakiś sposób 
lepiej odpowiadają strukturze klastrów niż dla zmiennej \texttt{symboling} np. dla zmiennej \texttt{num.of.cylinders} zdecydowana część lewej (z dwóch) gałęzi dendrogramu należy do kategorii oznaczonej kolorem zielonnym, z kolei da zmiennej \texttt{fuel.system}, zdecydowana większość prawej gałezi dendrogramu należy do etykietki oznaczonej kolorem różowym.
\par
Stworzone zostały one także dendrogramy porównujące z kategoriami pozostałych zmiennych jakościowych. Wykresy te umieszczone zostały w sekcji Dodatek. 

\par Dodatkowo, warto przypomnieć fakt, że wszystkie zmienne numeryczne były standaryzowane przed użyciem. Dlaczego tak zrobiliśmy? Poniżej odpowiedź na rysunkach \ref{fig:plot_AGNES_UNSCALED_1}, \ref{fig:plot_AGNES_UNSCALED_2} i \ref{fig:plot_AGNES_UNSCALED_3}. Przedstawione są tam porównawcze dendrogramy między użyciem ustandaryzowanych i oryginalnych danych liczbowych dla każdej z trzech wcześniej używanych przez nas metod łączenia.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_UNSCALED_1-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage}\label{fig:plot_AGNES_UNSCALED_1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_UNSCALED_2-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage}\label{fig:plot_AGNES_UNSCALED_2}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_UNSCALED_3-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage]{dendrogram dla metody AGNES dla zmiennych ilościowych z wykorzystaniem complete linkage}\label{fig:plot_AGNES_UNSCALED_3}
\end{figure}

\end{knitrout}

Widzimy na powyższych $3$ rysunków, że dendrogramy uzyskane na podstawie macierzy odmienności uzsykanych na bazie danych niestandaryzowanych mają zupełnie inne struktury. Bez zastosowania standaryzacji mamy oczywiście inną skalę dla odległości, co przekłada się na wygląd dendrogramów.



\subsubsection{DIANA}

Kolejną metodą, której użyjemy jest $DIANA$ (ang. DIviseve ANAlysis). Jest to przykład metody dzielącej, która działają niejako "odwrotnie" niż metody aglomeracyjne tzn. na początku wszystke obiekty tworzą jedno duże skupienie, które jest potem dzielone, tak aby otrzymać jednorodne klastry. $DIANA$, podobnie jak $AGNES$ potrzebuje na wejściu tylko macierz odmienności, więc oczywiście możemy używać zmiennych jakościowych jak i danych mieszanego typu.

\par Na początek porównamy metodę $DIANA$ dla wszystkich zmiennych i tylko dla zmiennych ilościowych. Na rysunkach \ref{fig:diana_all} i \ref{fig:diana_num} przedstawione zostały takie dendrogramy. 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/diana_all-1} 

}

\caption[dendrogram dla metody DIANA dla wszystkich zmiennych]{dendrogram dla metody DIANA dla wszystkich zmiennych}\label{fig:diana_all}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/diana_num-1} 

}

\caption[dendrogram dla metody DIANA dla zmiennych ilościowych]{dendrogram dla metody DIANA dla zmiennych ilościowych}\label{fig:diana_num}
\end{figure}

\end{knitrout}

Widzimy na dwóch powyższych rysunkach, że otrzymane dendogramy znacząco się różnią strukturą. Najwżniejszą obserwacją jest fakt, że w przypadku użycia wszystkich zmiennych, najbardziej wyraźny jest podział na $k=3$ klastry, natomiast dla zmiennych numerycznych są $k=2$ klastry. Możemy także zauważyć, że  wobu przypadkach, metoda $DIANA$ zwraca rezultaty podobne do algorytmu $AGNES$ z metodą łączenia klastrów complete linkage. 

\par  Ciekawym graficznym sposobem, żeby porównać $2$ dendrogramy jest tzw. tanglegram, który umieszcza $2$ dendrogramy w pozycji na przeciwko siebie, a na środku umieszcza linie łączące te same obiekty między dwoma dendrogramami. Poniżej na rysunkach \ref{fig:tanglegram1} i \ref{fig:tanglegram2} umieszczone zostały tanglegramy dla metody $DIANA$ i $AGNES$ (complete linkage) dla wszystkich zmiennych i dla zmiennych numerycznych odpowiednio. Co więcej, na owich tanglegramach zilustrowany został podziała na klastry, gdzie $k=3$ w przypadku wszystkich zmiennych i $k=2$ dla zmiennych numerycznych. Tanglegramy zostały stworzone za pomocą funkcji \texttt{tanglegram} z pakietu \textbf{dendextend}.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/tanglegram1-1} 

}

\caption[Tanglegram dla metod DIANA i AGNES dla wszystkich zmiennych]{Tanglegram dla metod DIANA i AGNES dla wszystkich zmiennych}\label{fig:tanglegram1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/tanglegram2-1} 

}

\caption[Tanglegram dla metod DIANA i AGNES dla zmiennych ilościowych]{Tanglegram dla metod DIANA i AGNES dla zmiennych ilościowych}\label{fig:tanglegram2}
\end{figure}

\end{knitrout}
Jak wyżymy powyżej, dla przypadku wszystkich zmiennych dendrogramy uzyskane metodami $DIANA$ i $AGNES$ są bliskie identyczności. Widzimy, że linie na środku tylko w pojedynczych przypadkach wychodzą z dendrogramu po lewej stronie z klastru o jednym kolorze, do dendrogramu po prawej stronie do klastra o kolorze innym. Dodatkowo, w obu przypadkach widzimy, że podział na $k=3$ klastry jest jak najbardziej optymalny. 
\par Nieco inaczej jest w przypadku zmiennych ilościowych, gdzie różnica między dendrogramami jest znacznie większa, ale wciąż otrzymane rezultaty możemy określić jako bardzo podobne.

\par W następnej części przeprowadzone zostały badania z różnymi niestandarowanymi podzbiorami cech, wybiórczo przez nas wybranymi. Poniżej na rysunkach \ref{fig:plot_DIANA_test_feat}, \ref{fig:plot_DIANA_symbo_2levels} przedstawione zostały dendrogramy uzyskane metodą $DIANA$ na bazie $4$ następujących zmiennych: \texttt{normalized.losses}, \texttt{fuel.system}, \texttt{num.of.doors}, \texttt{length}. Gałęzie dendrogramu zostały pokolorowane przyjmując podział na $k=2$ klastry, a na dole etykietki obiektów zostały pokolorowane zgodnie z etykietkami zmiennej \texttt{symboling}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_test_feat-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody DIANA, a 5 grup zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length]{Klasteryzacja na 2 grupy za pomocą metody DIANA, a 5 grup zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length}\label{fig:plot_DIANA_test_feat}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_symbo_2levels-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody DIANA, a 2 grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length]{Klasteryzacja na 2 grupy za pomocą metody DIANA, a 2 grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length}\label{fig:plot_DIANA_symbo_2levels}
\end{figure}

\end{knitrout}

Jak możemy zaobserwować powyżej $DIANA$ użyta dla $4$ wybranych zmiennych zwróciła bardzo ciekawe rezultaty. Mianowicie widzimy, że optymalną liczbą klastrów jest $k=2$, i to własnie dla tego podziału widzimy, że $2$ rozdzielone partycje dobrze oddają podział na $2$ grupy zmiennej \texttt{symboling} odpowiadające za mniej oraz bardziej ryzykowne auta. Żeby potwierdzić tezę, że jest to kwestia wyboru cech, poniżej na rysunku \ref{fig:plot_DIANA_symbo_2levels_num_all} przedstawione zostały analogiczne dendrogramy dla odpowiednio zmiennych numerycznych oraz wszystkich zmiennych z zaznaczonymi etykietkami zmiennej objaśnianej \texttt{symboling} dla podziału na $2$ kategorie, a także z zaznaczonym podziałem na $2$ klastry w dendrogramie.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_symbo_2levels_num_all-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length]{Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length}\label{fig:plot_DIANA_symbo_2levels_num_all}
\end{figure}

\end{knitrout}

Jasno widzimy, że podzbiory zawierające większą część zmiennych nie poradziły sobie tak dobrze z separacją wybranych grup zmiennej \texttt{symboling}.
\par Następnie porównamy czy algorytm $AGNES$ z metodą łączenia grup complete linkage działa równie dobrze biorąc pod uwagę tylko te same $4$ zmienne: \texttt{normalized.losses}, \texttt{fuel.system}, \texttt{num.of.doors}, \texttt{length}. Na rysunku \ref{fig:plot_AGNES_symbo_2levels} przedstawiony został dendrogram odpowiadający temu wyborowi cech.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_symbo_2levels-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody AGNES, a dwie grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length]{Klasteryzacja na 2 grupy za pomocą metody AGNES, a dwie grupy zmiennej objaśnianej dla zmiennych normalized.losses, fuel.system, num.of.doors, length}\label{fig:plot_AGNES_symbo_2levels}
\end{figure}

\end{knitrout}

Jak widzimy powyżej, ten dendrogram jest dużo bardziej zbliżony do analogicznego dla metody $DIANA$. Widzimy, że $2$ grupy zmiennej \texttt{symboling} są dobrze uchwycone. Żeby dokładniej porównać te $2$ dendrogramy, ponownie użyliśmy tanglegramu. Wyniki przedstawione zostały na rysunku \ref{fig:tanglegram3}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/tanglegram3-1} 

}

\caption[Tanglegram dla metod DIANA i AGNES dla zmiennych normalized.losses, fuel.system, num.of.doors, length]{Tanglegram dla metod DIANA i AGNES dla zmiennych normalized.losses, fuel.system, num.of.doors, length}\label{fig:tanglegram3}
\end{figure}

\end{knitrout}

Z rysunku powyżej możemy stwierdzić, że sprawdzają się nasze przypuszczenia dotyczące podobieństwa rezultatów metod $DIANA$ i $AGNES$. Widzimy, że podział na $2$ klastry jest bardzo podobny, prawie identyczny.

\par Już wcześniej wspomnieliśmy o współczynniku $AC$. Dodajmy, że dla metod dzielących istnieje analogiczny współczynnik - $DC$ (divisive coefficient). Podobnie, jak w wypadku $AC$, tutaj również wartość im bliższa $1$, tym bardziej istotny jest podział. Poniżej przedstawione zostały współczynniki $AC$ i $DC$ dla metod $AGNES$ i $DIANA$ odpowiednio.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Współczynnik AC wynosi: 0.986"
## [1] "Współczynnik DC wynosi: 0.984"
\end{verbatim}
\end{kframe}
\end{knitrout}
I widzimy, że są one bardzo blisko $1$, co pozwala nam uznać oba podziały za istotne.
\par Poniżej załączone zostały także fragmenty kodu z czymś na wzór tabeli kontygencji. UWAGA: etykietki klastrów przypisane są "na odwrót" tzn. poprawnie zaklsyfikowane obiekty nie leżą na głównej diagonali, a na drugiej!

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#funkcja zwracająca etykiety klastrów}
\hlkwd{cutree}\hlstd{(agnes_test,} \hlkwc{k}\hlstd{=}\hlnum{2}\hlstd{)[}\hlnum{1}\hlopt{:}\hlnum{15}\hlstd{]}
\end{alltt}
\begin{verbatim}
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 
##  1  1  1  2  2  1  2  2  2  1  1  2  1  2  2
\end{verbatim}
\begin{alltt}
\hlkwd{table}\hlstd{(symboling_custom,} \hlkwd{cutree}\hlstd{(agnes_test,} \hlkwc{k}\hlstd{=}\hlnum{2}\hlstd{))} \hlcom{#klastry sa odwrotnie!!}
\end{alltt}
\begin{verbatim}
##                 
## symboling_custom  1  2
##       -1 or 0     8 81
##       1 , 2 or 3 83 30
\end{verbatim}
\begin{alltt}
\hlcom{#analogicznie dla Diany}
\hlkwd{cutree}\hlstd{(diana_test,} \hlkwc{k}\hlstd{=}\hlnum{2}\hlstd{)[}\hlnum{1}\hlopt{:}\hlnum{15}\hlstd{]}
\end{alltt}
\begin{verbatim}
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 
##  1  1  1  2  2  1  2  2  2  1  1  2  1  2  2
\end{verbatim}
\begin{alltt}
\hlkwd{table}\hlstd{(symboling_custom,} \hlkwd{cutree}\hlstd{(diana_test,} \hlkwc{k}\hlstd{=}\hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
##                 
## symboling_custom  1  2
##       -1 or 0     7 82
##       1 , 2 or 3 82 31
\end{verbatim}
\end{kframe}
\end{knitrout}
Poniżej przedstawione zostały także wyniki dokładności dla obu tych metod:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] "Dokładność klasyfikacji dla metody AGNES: 0.81"
## [1] "Dokładność klasyfikacji dla metody DIANA: 0.81"
\end{verbatim}
\end{kframe}
\end{knitrout}
jak widzimy, obie metody zwróciły dokładnie ten sam poziom dokładności $0.81$, który możemy uznać za wysoki.

\par Jak pamiętamy, na początku stworzyliśmy zmienną jakościową \texttt{price}, na bazie oryginalnej. Na rysunku \ref{fig:plot_DIANA_price_factor} przedstawione jest porównanie klasteryzacji na $4$ grupy za pomocą metody $DIANA$ z rzeczywistymi etykietkami naszej zmodyfikowanej zmiennej \texttt{price}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_price_factor-1} 

}

\caption[Klasteryzacja na 4 grupy za pomocą metody DIANA, a dwie grupy zmiennej price]{Klasteryzacja na 4 grupy za pomocą metody DIANA, a dwie grupy zmiennej price}\label{fig:plot_DIANA_price_factor}
\end{figure}

\end{knitrout}
Widzimy, że wyniki naszego małego ekspetymentu nie są zbyt satysfakcjonujące. Nie możemy zauważyć, żadnego porządku i powiązania między otrzymanymi klastrami, a rzeczywistymi wartościami zmiennej \texttt{price}.

\par Sprawdzimy jeszcze działanie metody $DIANA$ dla podzbioru $4$ innych zmiennych. Przetestowanych zostało wiele podzbiorów zmiennych od $2$ do $5$ elementowych, mogę stwierdzić jedynie, że dla żadnego podzbioru nie uzyskaliśmy dokładności większej niż $0.81$, a przeważnie kończyło się na gorszych rezultatach. Dodatkowo wartym podkreślenia jest fakt, że przy testowaniu ręcznie "istotności" zmiennych, najważniejszą wydawała się \texttt{num.of.doors}, od której wystepowania lub jego braku w rozważanym podzbiorze cech zależało najwięcej. Poniżej na rysunkach \ref{fig:plot_DIANA_test_feat_2}, \ref{fig:plot_DIANA_test_feat_2_2levels} umieszczone zostały przykładowe dendrogramy dla zmiennych \texttt{price}, \texttt{height}, \texttt{curb.weight}, \texttt{wheel.base} i \texttt{peak.rpm}. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_test_feat_2-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych price, height, curb.weight, wheel.base i peak.rpm]{Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych price, height, curb.weight, wheel.base i peak.rpm}\label{fig:plot_DIANA_test_feat_2}
\end{figure}

\begin{kframe}\begin{verbatim}
## [1] "Dokładność dla zmiennej symboling przyjmującej tylko dwie wartości: 0.59"
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_DIANA_symbo_test2_2levels-1} 

}

\caption[Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych price, height, curb.weight, wheel.base i peak.rpm]{Klasteryzacja na 2 grupy za pomocą metody DIANA, a dwie grupy zmiennej objaśnianej dla zmiennych price, height, curb.weight, wheel.base i peak.rpm}\label{fig:plot_DIANA_symbo_test2_2levels}
\end{figure}

\end{knitrout}

Z powyżych rysunków widzimy jednoznacznie, że dla tego podzbioru wyniki nie są już tak zadowalające. 

\section{Redukcja wymiaru}

Redukcja wymiaru jest bardzo ważną częścią procesu data mining. Umożliwia ona m.in wizualizacje wyników na wykresach (dzięki sprowadzeniu do danych dwuwymiarowych), eliminacje nadmiarowej informacji czy identyfikacje struktury zależności w danych. Metody redukcji wymiaru mogą należeć zarówno do uczenia nienadzorowanego jak i do uczenia nadzorowanego w zależności od konkretnej metody. Jednakże dwie wybrane przez nas metody: $PCA$ i $MDS$ są przykładami uczenia nienadzorowanego.

\subsection{PCA - analiza składowych głównych}
Pierwszą metodą redukcji wymiaru wybraną przez nas jest $PCA$. Streszczając, poszukuje ona zbioru złożonej z mniejszej liczby zmiennych, dzięki któremu uda się nam zachować najważniejsze własności wyjściowych danych. Metoda ta działa bardzo dobrze zwłaszcza, gdy w analizowanych danych występują silnie skorelowane zmiene. Dodać trzeba, że $PCA$ działa tylko na zmiennych ilościowych. Metoda ta działa w ten sposób, że wybieramy kilka pierwszych składowych głównych, które wyjasniają odpowiednio dużą część całkowitej zmienności danych. Więcej o $PCA$ tutaj: \cite{PCA}. 




Do implementacji metody $PCA$ w \textbf{R} będziemy używać funkcji \texttt{PCA} \cite{PCA_R} z pakietu \textbf{FactoMiner}. Poniżej przedstawiony został zbiór elementów zwracany przez tą funkcję. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 202 individuals, described by 15 variables
## *The results are available in the following objects:
## 
##    name               description                          
## 1  "$eig"             "eigenvalues"                        
## 2  "$var"             "results for the variables"          
## 3  "$var$coord"       "coord. for the variables"           
## 4  "$var$cor"         "correlations variables - dimensions"
## 5  "$var$cos2"        "cos2 for the variables"             
## 6  "$var$contrib"     "contributions of the variables"     
## 7  "$ind"             "results for the individuals"        
## 8  "$ind$coord"       "coord. for the individuals"         
## 9  "$ind$cos2"        "cos2 for the individuals"           
## 10 "$ind$contrib"     "contributions of the individuals"   
## 11 "$call"            "summary statistics"                 
## 12 "$call$centre"     "mean of the variables"              
## 13 "$call$ecart.type" "standard error of the variables"    
## 14 "$call$row.w"      "weights for the individuals"        
## 15 "$call$col.w"      "weights for the variables"
\end{verbatim}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# starting httpd help server ... done}}\end{kframe}
\end{knitrout}

Poniżej przedstawiony został fragment kodu z wartościami własnymi macierzy kowariancji oraz wariancje składowych głównych.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##        eigenvalue variance.percent cumulative.variance.percent
## Dim.1  7.38694501       49.2463001                    49.24630
## Dim.2  2.50935638       16.7290425                    65.97534
## Dim.3  1.25624854        8.3749903                    74.35033
## Dim.4  0.89483536        5.9655691                    80.31590
## Dim.5  0.81373679        5.4249119                    85.74081
## Dim.6  0.59111185        3.9407456                    89.68156
## Dim.7  0.43896423        2.9264282                    92.60799
## Dim.8  0.31206672        2.0804448                    94.68843
## Dim.9  0.28411186        1.8940790                    96.58251
## Dim.10 0.17076900        1.1384600                    97.72097
## Dim.11 0.11708338        0.7805559                    98.50153
## Dim.12 0.08972921        0.5981948                    99.09972
## Dim.13 0.06540077        0.4360051                    99.53573
## Dim.14 0.05072099        0.3381399                    99.87387
## Dim.15 0.01891992        0.1261328                   100.00000
\end{verbatim}
\end{kframe}
\end{knitrout}

Poniżej na rysunku \ref{fig:plot_PCA2} zilustrowane zostały wariancje składowych głównych.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA2-1} 

}

\caption[Wariancje składowych głównych w metodzie PCA]{Wariancje składowych głównych w metodzie PCA}\label{fig:plot_PCA2}
\end{figure}

\end{knitrout}

Jednakże lepszym sposobem analizy jest porównanie skumulowanej wariancji pierwszych $k$ składowych głównych. Poniżej na rysunku \ref{fig:plot_PCA3} został przedstawiony własnie taki wykres, z dodaną linią przy wartości $80$ procent.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA3-1} 

}

\caption[Skumulowana wariancja pierwszych k składowych głównych]{Skumulowana wariancja pierwszych k składowych głównych}\label{fig:plot_PCA3}
\end{figure}

\end{knitrout}
Widzimy, że pierwsze $4$ składowe główne odpowiadają za około $80$ procent całej wariancji.  
\par Poniżej przedstawione są ładunki poszczególnych zmiennych dla pierwszych $5$ składowych głównych. Jeszcze niżej, na rysunku \ref{fig:plot_PCA4} przedstawione zostały korelacje zmienne z poszczególnymi składowymi głównymi.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##                         Dim.1       Dim.2       Dim.3       Dim.4        Dim.5
## normalized.losses  0.04299907 -0.31752013  0.39308333  0.02326830  0.750876807
## horsepower         0.29573530 -0.27841980  0.04641015 -0.13810572 -0.167054028
## peak.rpm          -0.07820718 -0.44266490 -0.07100704  0.46571636  0.057994277
## city.mpg          -0.30627911  0.26108699  0.14884793 -0.12846194  0.031987432
## highway.mpg       -0.31615699  0.20889895  0.15145551 -0.13500816  0.021247414
## price              0.31657394 -0.05489313  0.08419581 -0.13868626  0.003855721
## curb.weight        0.35557675  0.05316525  0.06062066 -0.01760471  0.029231152
## engine.size        0.32248581 -0.04626442  0.16134351 -0.22733288 -0.171440920
## bore               0.26202792  0.01022530 -0.18021122 -0.39205984 -0.106300039
## stroke             0.05587999  0.06965573  0.65210368  0.44284361 -0.491948674
## compression.ratio  0.01653042  0.44632328  0.38841240 -0.20273186  0.226085850
## wheel.base         0.28689080  0.28215058 -0.04333975  0.26158000  0.167102255
## length             0.32980179  0.16160441 -0.06045406  0.17627930  0.132572920
## width              0.32715682  0.08929685  0.09276061  0.07870337  0.090117093
## height             0.10000317  0.43366573 -0.36562623  0.40115539  0.123003376
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA4-1} 

}

\caption[Skumulowana wariancja pierwszych k składowych głównych]{Skumulowana wariancja pierwszych k składowych głównych}\label{fig:plot_PCA4}
\end{figure}

\end{knitrout}

Widzimy, że aż $10$ na $15$ zmiennych numerycznych jest silnie skorelowanych już z pierwsza składową główną. 
\par Poniżej na rysunku \ref{fig:plot_PCA5} zobrazowane zostały wkłady poszczególnych zmiennych w pierwsze $3$ składowe główne.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA5-1} 

}

\caption[Skumulowana wariancja pierwszych k składowych głównych]{Skumulowana wariancja pierwszych k składowych głównych}\label{fig:plot_PCA5}
\end{figure}

\end{knitrout}
Na wykresie powyżej widzimy, że największy wkład w pierwszą składową główną ma zmienna \texttt{curb.weight}. Oprócz niej największy wkład mają \texttt{length}, 
\texttt{width} i \texttt{engine.size}, z kolei zdecydowanie najmniejszy wpływ mają zmienne \texttt{height}, \texttt{peak.rpm}, \texttt{stroke}, \texttt{normalized.losses} oraz \texttt{compression.ratio}. Natomiast w przypadku drugiej składowej głównej, największy wpływ mają \texttt{compression.ratio}, \texttt{peak.rpm} i \texttt{height}, czyli zmienne, które miały znikomy wpływ na pierwszą składową. W trzeciej składowej głównej zdecydowanie największą rolę odgrywa zmienna \texttt{stroke}, która ma bardzo mały wkład w dwie pierwsze składowe główne.

\par
Z kolei poniżej na rysunku \ref{fig:plot_PCA6}, zamieszczony został wykres punktowy, który przedstawia rozrzut poszczególnych obiektów w przestrzeni dwóch pierwszych składowych głównych.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA6-1} 

}

\caption[Wykres rozrzutu w przestrzeni dwóch pierwszych składowych głównych]{Wykres rozrzutu w przestrzeni dwóch pierwszych składowych głównych}\label{fig:plot_PCA6}
\end{figure}

\end{knitrout}


\par
Poniżej na rysunku \ref{fig:plot_PCA7}, zamieszczony został wykres, który przedstawia rozmieszczenie poszczególnych zmiennych w przestrzeni dwóch pierwszych składowych głównych.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA7-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA7}
\end{figure}

\end{knitrout}

Teraz przejdziemy do porównania rezultatów otrzymanych metodą $PCA$ (konketnie pierwszych $2$ składowych) z rzeczywistymi wartościami zmiennych jakościowych. Na początek, na rysunkach \ref{fig:plot_PCA_symbo1}, \ref{fig:plot_PCA_symbo2} przedstawione zostało porównanie ze zmienną \texttt{symboling}.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_symbo1-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_symbo1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_symbo2-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_symbo2}
\end{figure}

\end{knitrout}

Mimo że możemy dostrzec pewne wzorce na powyższych rysunkach, to możemy stwierdzić, że kategorie zmiennej \texttt{symboling} nie zostały prawidłowo wychwycone.
\par Poniżej, na rysunkach \ref{fig:plot_PCA_drive.wheels}, \ref{fig:plot_PCA_fuel.system}, \ref{fig:plot_PCA_num.of.doors} przedstawione zostały analogiczne wizualizacje dla innych zmiennych jakościowych: \texttt{drive.wheels}, \texttt{fuel.system} i \texttt{num.of.doors}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_drive_wheels-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_drive.wheels}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Too few points to calculate an ellipse\\\#\# Too few points to calculate an ellipse\\\#\# Too few points to calculate an ellipse}}\end{kframe}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_fuel_system-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_fuel.system}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_num_of_doors-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_num.of.doors}
\end{figure}

\end{knitrout}
Z rysunków powyżej możemy wywnioskować, że żadna ze zmiennych jakościowych nie została odseparowana w bardzo dobry sposób. Możemy wychwycić pewne wzorce, jak np. dla \texttt{fuel.system}$=$"idi", ale to wszystko.

\par Ponadto, na rysunku \ref{fig:plot_PCA_price_highway.mpg} przedstawione zostały $2$ wykresy punktowe porównujące rozrzut zmiennych ilościowych \texttt{price} i \texttt{highway.mpg} tzn. wszystkie obiekty zaznaczone są kolorami zgodnymi z odpowiednimi wartościami tych zmiennych.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_price_highway_mpg-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_price_highway.mpg}
\end{figure}

\end{knitrout}
Możemy zaobserwować, że zmienna \texttt{price} skorelowana jest dodatnio z pierwszą składową główną, a zmienna \texttt{highway.mpg} skorelowana jest ujemnie z tą samą składową.


Poniżej na rysunku \ref{fig:plot_PCA_compression.ratio_peak.rpm} analogiczne wykresy dla zmiennych \texttt{compression.ratio} i \texttt{peak.rpm}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_compression_ratio_peak_rpm-1} 

}

\caption[wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych]{wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych}\label{fig:plot_PCA_compression.ratio_peak.rpm}
\end{figure}

\end{knitrout}
Widzimy powyżej, że zmienna \texttt{compression.ratio} skorelowana jest dodatnio z drugą składową głowną, a zmienna \texttt{peak.rpm} skorelowana ujemnie. Wszystkie te wnioski są potwierdzeniem danych, które dostaliśmy na rysunku \ref{fig:plot_PCA7}. Uwaga: przy zmiennych ilościowych, które porównywaliśmy na $2$ powyższych wykresach, należy pamiętać, że nie usuwaliśmy ich z podzbioru wybranych cech, co mogło mieć duże znaczenie na rezultaty.


\subsection{MDS - skalowanie wielowymiarowe}

Drugą metodą redukcji wymiaru użytą przez nas będzie $MDS$ czyli skalowanie wielowymiarowe. Metoda ta polega na odtworzeniu odległości (lub ogólniej odmienności) między obiektami w nowej przestrzeni o mniejszym wymiarze. Główną zaletą metody $MDS$ jest właśnie fakt, że na wejściu możemy jej dostarczyć dowolną macierz odmienności, czyli działa ona także na zmiennych jakościowych. Więcej o metodzie $MDS$ tutaj: \cite{MDS}.
\par Na rysunku \ref{fig:plot_MDS_rozrzut} przedstawiony został wykres rozrzutu przy skalowaniu wielowymiarowym do wymiaru $k=2$ dla wszystkich zmiennych. Do implementacji $MDS$ w \textbf{R} użyliśmy funkcji \texttt{cmdscale} \cite{cmdscale} z pakietu \textbf{stats}.



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_MDS_rozrzut-1} 

}

\caption[Wykres rozrzutu przy skalowaniu wielowymiarowym dla k=2]{Wykres rozrzutu przy skalowaniu wielowymiarowym dla k=2}\label{fig:plot_MDS_rozrzut}
\end{figure}

\end{knitrout}

Poniżej na rysunku \ref{fig:plot_PCA_MDS_symbo} przedstawione zostało porównanie metod: $MDS$ dla wszystkich zmiennych, $MDS$ dla zmiennych numerycznych i metody $PCA$. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_symbo-1} 

}

\caption[Porównanie metod MDS i PCA]{Porównanie metod MDS i PCA}\label{fig:plot_PCA_MDS_symbo}
\end{figure}

\end{knitrout}

Warto zauważyć znany fakt, że dla wybranych zmiennych numerycznych metoda MDS z wymiarem $k=2$ odpowiada $2$ pierwszym składowym głównym z metody PCA. (Wykresy te są symetryczne względem osi $y$). Dla pozostałych zmiennych poniżej przedstawionie zostanie już tylko metoda $MDS$ na wszystkich zmiennych, oraz metoda $MDS$ dla zmiennych numerycznych, która odpowiada metodzie $PCA$.
\par Poniżej na rysunku \ref{fig:plot_PCA_MDS_symbo2} zostały przedstawione te same wykresy co powyżej, ale dodatkowo z zaznaczonymi faktycznymi kategoriami zmiennej \texttt{symboling} w wersji z jej dwoma etykietkami.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_symbo2-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej symboling]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej symboling}\label{fig:plot_PCA_MDS_symbo2}
\end{figure}

\end{knitrout}

NA podstawie rysunku powyżej możemy stwierdzić, że widać pewien wzorzec rozmieszczenia poszczególnych kategorii zmiennej \texttt{symboling}, jednakże z drugiej strony nie możemy mówić też o jednoznacznej bezbłędnej separacji obu obszarów.

\par Poniżej na rysunkach \ref{fig:plot_PCA_MDS_drive}, \ref{fig:plot_PCA_MDS_num.of.doors}, \ref{fig:plot_PCA_MDS_body.style}, \ref{fig:plot_PCA_MDS_fuel.system} i \ref{fig:plot_PCA_MDS_price_factor} przedstawione zostały analogiczne wykresy dla innych zmiennych jakościowych. Warto podkreślić, że dla każdej z nich stworzona została odpowiednia macierz odmienności nieuwzględniająca tej konkretnej zmiennej.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_drive-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej drive.wheels]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej drive.wheels}\label{fig:plot_PCA_MDS_drive}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_num_of_doors-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej num.of.doors]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej num.of.doors}\label{fig:plot_PCA_MDS_num.of.doors}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_body_style-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej body.style]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej body.style}\label{fig:plot_PCA_MDS_body.style}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_fuel_system-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej fuel.system]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej fuel.system}\label{fig:plot_PCA_MDS_fuel.system}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_PCA_MDS_price_factor-1} 

}

\caption[Porównanie metod MDS z rzeczywistymi wartościami zmiennej price (typu factor)]{Porównanie metod MDS z rzeczywistymi wartościami zmiennej price (typu factor)}\label{fig:plot_PCA_MDS_price_factor}
\end{figure}

\end{knitrout}

Na powyższych wykresach, widzimy, że dla wszystkich wybranych zmiennych widać pewne wzorce na wykresach rozrzutu, odnośnie odseparowania ich poszczeólnych kategorii, najgorzej prawdopodobnie dla zmiennej \texttt{body.style}. Innym wnioskiem może być to, że dla wszystkich zmiennych metoda $MDS$ użyta na bazie wszystkich zmiennych zwraca lepsze rezultaty, potencjalne partycje są bardziej widoczne niż w przypadku tylko zmiennych ilościowych.

\subsection{Zastosowanie metod redukcji wymiaru przy klasteryzacji}

Oczywiście, ważnym zastosowaniem metod redukcji wymiaru, jest po prostu użycie ich w celu klasyfikacji czy klasteryzacji.
\par Poniżej na rysunku \ref{fig:PAM_MDS_Silhouette} przedstawione został wykres Silhouette'a dla metody PAM użytej na oryginalnych danych.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##   cluster size ave.sil.width
## 1       1  102          0.09
## 2       2  100          0.35
\end{verbatim}
\end{kframe}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/PAM_MDS_Silhouette-1} 

}

\caption[PAM na bazie MDS z k=2]{PAM na bazie MDS z k=2}\label{fig:PAM_MDS_Silhouette}
\end{figure}

\end{knitrout}

Następnie, na rysunku \ref{fig:PAM_MDS_wizualizacja} przedstawiona została Wizualizacja wyników grupowania metodą PAM po użyciu metody $MDS$ dla wymiaru $k=2$. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/PAM_MDS_wizualizacja-1} 

}

\caption[Wizualizacja wyników grupowania - metoda PAM po użyciu metody MDS]{Wizualizacja wyników grupowania - metoda PAM po użyciu metody MDS}\label{fig:PAM_MDS_wizualizacja}
\end{figure}

\end{knitrout}
Powyżej widzimy dwa dobrze odseparowane skupienia, z małymi wyjątkami. Jednakże to jeszcze nam nic nie mówi, postaramy się zbadać charakterystykę obu tych skupień pod kątem innych zmiennych.
\par Na początku, na rysunku \ref{fig:PAM_MDS_tabelki} przedstawione zostały wykresy słupkowe dla zmiennych jakościowych, pogrupowane ze względu na oba klastry uzyskane przy użyciu $PAM$.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/PAM_MDS_tabelki-1} 

}

\caption[Porównanie występowania zmiennych jakościowych w dwóch klastrach po użyciu metod PAM i MDS]{Porównanie występowania zmiennych jakościowych w dwóch klastrach po użyciu metod PAM i MDS}\label{fig:PAM_MDS_tabelki}
\end{figure}

\end{knitrout}

Jak możemy zaobserwować powyżej, rozkłady występowań poszczególnych zmiennych jakościowych różnią się w zależności od klastrów. Najbardziej widać to dla zmiennych \texttt{fuel.system}, \texttt{drive.wheels}, \texttt{body.style} i \texttt{num.of.doors}.

\par Poniżej na rysunku \ref{fig:PAM_MDS_boxploty} przedstawione zostały wykresy pudełkowe dla zmiennych ilościowych pogrupowane ze względu na klastry uzyskane metodą $PAM$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/PAM_MDS_boxploty-1} 

}

\caption[Porównanie zmiennych ilościowych dla dwóch klastrów po użyciu metod PAM i MDS]{Porównanie zmiennych ilościowych dla dwóch klastrów po użyciu metod PAM i MDS}\label{fig:PAM_MDS_boxploty}
\end{figure}

\end{knitrout}
Na rysunkach powyżej widzimy wyraźne różnice dla wszystkich zmiennych ilościowych oprócz \texttt{stroke} i \texttt{compression.ratio} ze względu na klastry uzyskane metodą $PAM$.

\subsection{Zastosowanie metod redukcji wymiaru przy klasyfikacji}

Metod redukcji wymiaru możemy także użyć w celu poprawy klasyfikacji. W szczególności, bardzo ciekawym zastosowaniem jest użycie metody $MDS$ na danych mieszanego typu, która zwraca nam dane liczbowe po to, żeby następnie użyć otrzymanych wyników do implementacji metod, które na wejściu przyjmują tylko zmienne ilościowe.
\par Przeprowadzliśmy symulacje dla metody $k$-najbliższych sąsiadów dla $k=5$ oraz $k=9$, dla metody $LDA$, oraz dla metody $KDA$. Użyliśmy skalowania wielowymiarowego do różnych wartości wymiarów, a nastepnie obliczyliśmy dokładność dla klasyfikacji zmiennej \texttt{symboling} i wyniki zostały przedstawione za pomocą wykresów pudełkowych na rysunkach \ref{fig:MDS_knn_boxploty1}, \ref{fig:MDS_knn_boxploty2}, \ref{fig:MDS_lda_boxplot} oraz \ref{fig:MDS_kda_boxplot}. UWAGA:redukcję wymiaru stosujemy dla całych danych (zbiór tereningowy + zbiór testowy)!




\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/MDS_knn_boxploty1-1} 

}

\caption[Porównanie metody MDS dla różnych wymiarów dla KNN z k=5]{Porównanie metody MDS dla różnych wymiarów dla KNN z k=5}\label{fig:MDS_knn_boxploty1}
\end{figure}

\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/MDS_knn_boxploty2-1} 

}

\caption[Porównanie metody MDS dla różnych wymiarów dla KNN z k=9]{Porównanie metody MDS dla różnych wymiarów dla KNN z k=9}\label{fig:MDS_knn_boxploty2}
\end{figure}

\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/MDS_lda_boxplot-1} 

}

\caption[Metoda MDS dla LDA]{Metoda MDS dla LDA}\label{fig:MDS_lda_boxplot}
\end{figure}

\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/MDS_kda_boxplot-1} 

}

\caption[Metoda MDS dla KDA]{Metoda MDS dla KDA}\label{fig:MDS_kda_boxplot}
\end{figure}

\end{knitrout}
Niestety, wyniki naszych symmulacji nie wypadły pomyślnie, na wszystkich wykresach powyżej widzimy, że wartości dokładności nie są zbyt dobre, wahają się one między $0.45$, a $0.6$, co daje gorsze rezultaty niż w przypadku użycia tych samych metod, bez wcześniejszego użycia metody redukcji wymiaru.


\section{Podsumowanie drugiej części projektu}

Na początku drugiej części projektu, w celu klasteryzacji użyliśmy metod grupujących $k-means$ i $PAM$, które zwracały sensowne podziały na partycje, jednakże nie miały one prawie nic wspólnego ze zmienną objaśnianą \texttt{symboling}. Algorytm $DBSCAN$ wypadł w naszym porównaniu prawdpodobnie najgorzej, przez niejednorodną gęstość punktów, albo nie umiał wychwycić różnych skupien, albo złączał je w jedną (za) dużą całość. Nie było nawet sensu porównywać ze zmienną \texttt{symboling}. Najlepsze rezultaty otrzymaliśmy za pomocą metod hierarchicznych, zwłaszcza tych wprowadzonych na $4$-elementowym podzbiorze cech składających się ze zmiennych \texttt{normalized.losses}, \texttt{fuel.system}, \texttt{num.of.doors} i \texttt{length}. 
\par Później zajeliśmy się redukcją wymiaru, która również dała nam kilka ciekawych wyników, przede wszystkim mogliśmy zwizualizować dużo więcej rzeczy przez zwinięcie wielu cech do wymiaru $k=2$, w szczególności ciekawe rezultaty widzieliśmy porównując $MDS$ dla wymiaru $k=2$ z etykietkami różnych zmiennych jakościowych, w tym dla nas najważniejszej - \texttt{symboling}. 
\par Następnie postaraliśmy się o wprowadzenie metod redukcji przy klasteryzacji oraz klasyfikacji. W przypadku grupowania, powiedzmy, że uzyskaliśmy jakies sensowne wyniki, $2$ otrzymane klastry różniły się pod względem charakterystyki, patrząc na różnice w rozkładach różnych zmiennych między partycjami. Natomiast w przypadku klasyfikacji, możemy mówić o rozczarowaniu, ponieważ wprowadzenie metod redukcji wymiaru, tylko pogorszyło dokładność klasyfikacji zmiennej \texttt{symboling}.

@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Dodatek}

\subsection{AGNES complete linkage - porównania z pozostałymi zmiennymi jakościowymi}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_make-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej make]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej make}\label{fig:plot_AGNES_Comparison_make}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_fuel_type-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej fuel.type]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej fuel.type}\label{fig:plot_AGNES_Comparison_fuel.type}
\end{figure}

\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_Aspiration-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej aspiration]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej aspiration}\label{fig:plot_AGNES_Comparison_Aspiration}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_num_of_doors-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej num.of.doors]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej num.of.doors}\label{fig:plot_AGNES_Comparison_num.of.doors}
\end{figure}

\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_body_style-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej body.style]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej body.style}\label{fig:plot_AGNES_Comparison_body.style}
\end{figure}

\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_engine_location-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej engine.location]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej engine.location}\label{fig:plot_AGNES_Comparison_engine.location}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plot_AGNES_Comparison_engine_type-1} 

}

\caption[dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej engine.type]{dendrogram dla metody AGNES dla zmiennych ilościowych z zaznaczeniem różnych kategorii zmiennej engine.type}\label{fig:plot_AGNES_Comparison_engine.type}
\end{figure}

\end{knitrout}

\subsection{dendrogramy AGNES - wszystkie zmienne}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/plots_AGNES_all-1} 

}

\caption[Porównanie wartości wskaźnika connectivity]{Porównanie wartości wskaźnika connectivity}\label{fig:plots_AGNES_all}
\end{figure}

\end{knitrout}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}
\bibitem{cramerV}
\begin{verbatim}https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\end{verbatim}
\bibitem{cramerVR}
https://www.rdocumentation.org/packages/rcompanion/versions/2.3.26/topics/cramerV
\bibitem{lmR}
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm
\bibitem{anovaR}
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/anova
\bibitem{KruskalR}
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kruskal.test
\bibitem{wiki}
\begin{verbatim}https://en.wikipedia.org/wiki/Effect_size\end{verbatim}
\bibitem{eta}
https://resources.nu.edu/statsresources/eta
\bibitem{omega}
https://cran.r-project.org/web/packages/effectsize/vignettes/interpret.html
\bibitem{Kruskal}
https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistics/nonparametrics/how-to/kruskal-wallis-test/interpret-the-results/key-results/
%\beg
\bibitem{Gini}
https://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore
\bibitem{kmeans}
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans
\bibitem{silhoutette}
\begin{verbatim}https://en.wikipedia.org/wiki/Silhouette_(clustering)\end{verbatim}
\bibitem{nbclust}
\begin{verbatim}https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust\end{verbatim}
\bibitem{daisy}
https://www.rdocumentation.org/packages/cluster/versions/2.1.4/topics/daisy
\bibitem{Gower}
https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553
\bibitem{factoextra}
https://cran.r-project.org/web/packages/factoextra/factoextra.pdf
\bibitem{Dunn}
\begin{verbatim}https://en.wikipedia.org/wiki/Dunn_index\end{verbatim}
\bibitem{DBSCAN}
https://en.wikipedia.org/wiki/DBSCAN
\bibitem{heatmap2}
https://www.rdocumentation.org/packages/gplots/versions/3.1.3/topics/heatmap.2
\bibitem{linkage}
\begin{verbatim}https://en.wikipedia.org/wiki/Hierarchical_clustering\end{verbatim}
\bibitem{agnes}
https://www.rdocumentation.org/packages/cluster/versions/2.1.4/topics/agnes
\bibitem{vizdend}
\begin{verbatim}https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_dend\end{verbatim}
\bibitem{PCA}
\begin{verbatim}https://pl.wikipedia.org/wiki/Analiza_g%C5%82%C3%B3wnych_sk%C5%82adowych\end{verbatim}
\bibitem{PCA_R}
https://www.rdocumentation.org/packages/FactoMineR/versions/2.8/topics/PCA
\bibitem{MDS}
\begin{verbatim}https://en.wikipedia.org/wiki/Multidimensional_scaling\end{verbatim}
\bibitem{cmdscale}
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cmdscale
\end{thebibliography}



\end{document}
